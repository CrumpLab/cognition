

# Computational models



<!--

 moved from computer metphor section in earlier information processing chapter.
 
 ## The computerization of cognition

Funny, I didn't think that [computerization](https://en.wikipedia.org/wiki/Computerize) was a word, but apparently it is, so we'll stick with it. The previous section invoked an industrial revolution metaphor of the assembly line in relation to the idea of mental processing stages. This section invokes a digital revolution metaphor of the computer in relation to cognitive processing. I mentioned earlier that there are several computer metaphors in cognition, and we will not have space to discuss all of them in this chapter, so this theme will keep popping up in later chapters.

### A brief history of computers

To understand computer metaphors for cognition it is necessary to know what a computer is and how it works, so here is a short history.

#### Analog Days

::: floatright25

**Computer Room, 1949**


\includegraphics[width=1\linewidth]{imgs/NACA_computers} 

:::

Computers existed in various forms before the digital computer was introduced. The main purpose of a computer is to compute things, and anything capable of computing can be called a computer. For example, the [first computer rooms](https://en.wikipedia.org/wiki/Computer_(occupation)) were full of people, often mainly women, who were hired as "computers" to do hand calculations. Importantly, the workers computed in a coordinated fashion. Complicated math problems that would take a single person too long to work out could be broken down into smaller units, and these smaller units could be spread across individuals in the room. In this way, the labor of computing a solution was divided, and everybody worked on parts of the problem in parallel.

::: floatright50

**Difference Engine**


\includegraphics[width=1\linewidth]{imgs/Babbage_diff} 

:::

There were also analog machines that could perform computations prior to the digital computer. For example, the [abacus](https://en.wikipedia.org/wiki/Abacus) can be used to perform some mathematical calculations. In 1882, [Charles Babbage](https://en.wikipedia.org/wiki/Charles_Babbage#Difference_engine) drew up plans to create a machine, called the difference engine, that could be used to compute values of polynomial functions. He made a prototype that was never finished, but his plans were clear enough that a modern version was assembled (1989-1991) and shown to perform calculations.

The theory of computing was also invented before digital computers, and in many ways is just as impressive, if not moreso than any physical computer. For example, in 1936 [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing#Early_computers_and_the_Turing_test) wrote a famous theoretical paper called, "On computable numbers, with an application to the Entscheidungsproblem". This paper described a simple process, now called a Turing machine, that could be used to perform a computation. It was simple enough that a person could perform the operations of the process on a piece of paper. More important, Turing proved that his theoretical machine was a universal computer-- it could compute all computable things. In other words, if a problem could be converted into a computational one, then the Turing machine could compute it. It wouldn't take too long before the principles of his theoretical machine were turned into reality. 

In case you are not familiar, you may be wondering what it means to be "computable". I'll return to this question after briefly discussing digital computers. After that we will be in a position to ask what it means to be computable, and what digital computers do when they compute something.

#### Digital Days

Wikipedia has decent coverage of the historical [development of computing hardware](https://en.wikipedia.org/wiki/History_of_computing_hardware), and I will refer you there for a more complete history of the advent of digital computers. A compressed version of the story is that electronic computing machines were developed around the years of world war II and used for code-breaking. Turing himself was involved in code-breaking efforts, and aided in the creation of electronic computers. The first "Turing-complete" computer was called [ENIAC](https://en.wikipedia.org/wiki/ENIAC) (Electronic Numerical Integrator and Computer) and completed in 1945. The important aspect was that ENIAC could be programmed in a general way, and like a Turing machine, was therefore capable of computing any computable problem.

In the early days, computers were very large devices that also filled whole rooms. Although human computing rooms were replaced, women continued to make important contributions to the development of computing technology. The basic architecture of modern computers as we know them has not changed very much in terms of the principles of operations. But, the parts have become much smaller and faster. Computer technology has been described by [Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law). Moore's law is the observation that the number of transistors in an integrated circuit doubles about every two years, which is an astounding exponential growth. The result has been that computers have become faster and faster, allowing them to compute more and more kinds of things in shorter amounts of time.

-->
