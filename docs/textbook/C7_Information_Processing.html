<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-09-01">

<title>cognition - 7 Information Processing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0PT8LKEC0K"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-0PT8LKEC0K', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="custom.css">
<meta property="og:title" content="cognition - 7 Information Processing">
<meta property="og:description" content="This chapter covers the concepts of processing stages, information, and capacity limitations, which became popular cognitive research topics around the 1950s and 60s.">
<meta property="og:image" content="https://crumplab.com/cognition/textbook/imgs/cover.jpg">
<meta property="og:site-name" content="cognition">
<meta name="twitter:title" content="cognition - 7 Information Processing">
<meta name="twitter:description" content="This chapter covers the concepts of processing stages, information, and capacity limitations, which became popular cognitive research topics around the 1950s and 60s.">
<meta name="twitter:image" content="https://crumplab.com/cognition/textbook/imgs/cover.jpg">
<meta name="twitter:creator" content="@MattCrumpLab">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../articles/course_docs/Fall_2022_Psyc_2530_syllabus.html" rel="" target="">
 <span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../textbook/index.html" rel="" target="">
 <span class="menu-text">Textbook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../articles/modules.html" rel="" target="">
 <span class="menu-text">Learning Modules</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-slides" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Slides</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-slides">    
        <li>
    <a class="dropdown-item" href="../slides/L0_Getting_started/L0_Getting_Started.html" rel="" target="">
 <span class="dropdown-text">Lecture 0 - Getting Started</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L1_Instances/L1_Instances.html" rel="" target="">
 <span class="dropdown-text">Lecture 1 - What is Cognition?</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L2_QALMRI/L2_QALMRI.html" rel="" target="">
 <span class="dropdown-text">Lecture 2 - Reading Research</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L3_Imagery/L3_Imagery.html" rel="" target="">
 <span class="dropdown-text">Lecture 3 - Mental Imagery</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L4_Eugenics/L4_Eugenics.html" rel="" target="">
 <span class="dropdown-text">Lecture 4 - Psychology and Eugenics</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L5_IQ/L5_IQ.html" rel="" target="">
 <span class="dropdown-text">Lecture 5 - Intelligence Testing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L6_Associations/L6_Associations.html" rel="" target="">
 <span class="dropdown-text">Lecture 6 - Associations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L8_Behaviorism/L8_Behaviorism.html" rel="" target="">
 <span class="dropdown-text">Lecture 8 - Behaviorism</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L9_Information/L9_Information.html" rel="" target="">
 <span class="dropdown-text">Lecture 9 - Information Processing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L10_Memory_1/L10_Memory_1.html" rel="" target="">
 <span class="dropdown-text">Lecture 10 - Memory I</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L11_Memory_2/L11_Memory_2.html" rel="" target="">
 <span class="dropdown-text">Lecture 11 - Memory II</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L12_Implicit/L12_Implicit.html" rel="" target="">
 <span class="dropdown-text">Lecture 12 - Implicit Influences</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L13_Attention/L13_Attention.html" rel="" target="">
 <span class="dropdown-text">Lecture 13 - Attention</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/Extra_Minerva/Extra_Minerva.html" rel="" target="">
 <span class="dropdown-text">Lecture 13.5 - Instance Theory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L14_Language/L14_Language.html" rel="" target="">
 <span class="dropdown-text">Lecture 14 - Language</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../slides/L15_JDM/L15_JDM.html" rel="" target="">
 <span class="dropdown-text">Lecture 15 - Judgment and Decision-making</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../articles/honors/honors.html" rel="" target="">
 <span class="menu-text">Honors</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../articles/course_docs/resources.html" rel="" target="">
 <span class="menu-text">Additional Resources</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../articles/oer/oer.html" rel="" target="">
 <span class="menu-text">OER</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/CrumpLab/cognition" rel="" target=""><i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../textbook/C7_Information_Processing.html">7 Information Processing</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../images/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Instances of Cognition</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/Preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/Reading_Formats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reading Formats</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C1_What_is_Cognition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 What is Cognition?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C2_Mental_Imagery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 Mental Imagery</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C3_Eugenics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Eugenics and Psychology</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C4_Intelligence_testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Intelligence Testing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C5_Associations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Associations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C6_Behaviorism.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Behaviorism</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C7_Information_Processing.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">7 Information Processing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C8_Memory_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Memory I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../textbook/C9_Memory_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9 Memory II</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#four-revolutions-industrial-technological-digital-and-cognitive" id="toc-four-revolutions-industrial-technological-digital-and-cognitive" class="nav-link active" data-scroll-target="#four-revolutions-industrial-technological-digital-and-cognitive"><span class="header-section-number">7.1</span> Four Revolutions: Industrial, Technological, Digital, and “Cognitive”</a>
  <ul>
  <li><a href="#assembly-line-metaphor-of-cognition" id="toc-assembly-line-metaphor-of-cognition" class="nav-link" data-scroll-target="#assembly-line-metaphor-of-cognition"><span class="header-section-number">7.1.1</span> Assembly-line metaphor of cognition</a></li>
  <li><a href="#telephone-metaphor-of-cognition" id="toc-telephone-metaphor-of-cognition" class="nav-link" data-scroll-target="#telephone-metaphor-of-cognition"><span class="header-section-number">7.1.2</span> Telephone metaphor of cognition</a></li>
  <li><a href="#computer-metaphor-of-cognition" id="toc-computer-metaphor-of-cognition" class="nav-link" data-scroll-target="#computer-metaphor-of-cognition"><span class="header-section-number">7.1.3</span> Computer metaphor of cognition</a></li>
  </ul></li>
  <li><a href="#the-mechanization-of-cognition" id="toc-the-mechanization-of-cognition" class="nav-link" data-scroll-target="#the-mechanization-of-cognition"><span class="header-section-number">7.2</span> The mechanization of cognition</a></li>
  <li><a href="#donders-processing-stages" id="toc-donders-processing-stages" class="nav-link" data-scroll-target="#donders-processing-stages"><span class="header-section-number">7.3</span> Donders’ Processing Stages</a>
  <ul>
  <li><a href="#donders-mental-chronometry-and-processing-stages" id="toc-donders-mental-chronometry-and-processing-stages" class="nav-link" data-scroll-target="#donders-mental-chronometry-and-processing-stages"><span class="header-section-number">7.3.1</span> Donders mental chronometry and processing stages</a></li>
  <li><a href="#physiological-reaction-time" id="toc-physiological-reaction-time" class="nav-link" data-scroll-target="#physiological-reaction-time"><span class="header-section-number">7.3.2</span> Physiological reaction time</a></li>
  <li><a href="#donders-mental-reaction-times" id="toc-donders-mental-reaction-times" class="nav-link" data-scroll-target="#donders-mental-reaction-times"><span class="header-section-number">7.3.3</span> Donders mental reaction times</a></li>
  <li><a href="#donders-subtractive-stage-logic" id="toc-donders-subtractive-stage-logic" class="nav-link" data-scroll-target="#donders-subtractive-stage-logic"><span class="header-section-number">7.3.4</span> Donders subtractive stage logic</a></li>
  </ul></li>
  <li><a href="#beyond-donders" id="toc-beyond-donders" class="nav-link" data-scroll-target="#beyond-donders"><span class="header-section-number">7.4</span> Beyond Donders</a>
  <ul>
  <li><a href="#subtractive-logic" id="toc-subtractive-logic" class="nav-link" data-scroll-target="#subtractive-logic"><span class="header-section-number">7.4.1</span> Subtractive logic</a></li>
  <li><a href="#reaction-time-research" id="toc-reaction-time-research" class="nav-link" data-scroll-target="#reaction-time-research"><span class="header-section-number">7.4.2</span> Reaction time research</a></li>
  <li><a href="#processing-stages" id="toc-processing-stages" class="nav-link" data-scroll-target="#processing-stages"><span class="header-section-number">7.4.3</span> Processing Stages</a></li>
  </ul></li>
  <li><a href="#cybernetics-and-the-macy-conferences" id="toc-cybernetics-and-the-macy-conferences" class="nav-link" data-scroll-target="#cybernetics-and-the-macy-conferences"><span class="header-section-number">7.5</span> Cybernetics and the Macy Conferences</a></li>
  <li><a href="#shannons-information-theory" id="toc-shannons-information-theory" class="nav-link" data-scroll-target="#shannons-information-theory"><span class="header-section-number">7.6</span> Shannon’s Information Theory</a>
  <ul>
  <li><a href="#information-channels" id="toc-information-channels" class="nav-link" data-scroll-target="#information-channels"><span class="header-section-number">7.6.1</span> Information channels</a></li>
  <li><a href="#measuring-information-h" id="toc-measuring-information-h" class="nav-link" data-scroll-target="#measuring-information-h"><span class="header-section-number">7.6.2</span> Measuring Information: H</a></li>
  <li><a href="#computing-h" id="toc-computing-h" class="nav-link" data-scroll-target="#computing-h"><span class="header-section-number">7.6.3</span> Computing H</a></li>
  <li><a href="#bits-of-information" id="toc-bits-of-information" class="nav-link" data-scroll-target="#bits-of-information"><span class="header-section-number">7.6.4</span> Bits of information</a></li>
  <li><a href="#h-bits-predictability-and-information" id="toc-h-bits-predictability-and-information" class="nav-link" data-scroll-target="#h-bits-predictability-and-information"><span class="header-section-number">7.6.5</span> H, Bits, predictability and Information</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">7.6.6</span> Summary</a></li>
  </ul></li>
  <li><a href="#hick-hyman-law" id="toc-hick-hyman-law" class="nav-link" data-scroll-target="#hick-hyman-law"><span class="header-section-number">7.7</span> Hick-Hyman “Law”</a>
  <ul>
  <li><a href="#choice-reaction-time" id="toc-choice-reaction-time" class="nav-link" data-scroll-target="#choice-reaction-time"><span class="header-section-number">7.7.1</span> Choice Reaction time</a></li>
  <li><a href="#the-number-of-alternatives-increases-choice-rt" id="toc-the-number-of-alternatives-increases-choice-rt" class="nav-link" data-scroll-target="#the-number-of-alternatives-increases-choice-rt"><span class="header-section-number">7.7.2</span> The number of alternatives increases choice-RT</a></li>
  <li><a href="#number-of-alternatives-or-information-in-the-message" id="toc-number-of-alternatives-or-information-in-the-message" class="nav-link" data-scroll-target="#number-of-alternatives-or-information-in-the-message"><span class="header-section-number">7.7.3</span> Number of alternatives or “Information” in the message?</a></li>
  <li><a href="#deconfounding-alternatives-from-information" id="toc-deconfounding-alternatives-from-information" class="nav-link" data-scroll-target="#deconfounding-alternatives-from-information"><span class="header-section-number">7.7.4</span> Deconfounding alternatives from information</a></li>
  <li><a href="#the-experiments" id="toc-the-experiments" class="nav-link" data-scroll-target="#the-experiments"><span class="header-section-number">7.7.5</span> The experiments</a></li>
  <li><a href="#implications-for-behaviorism" id="toc-implications-for-behaviorism" class="nav-link" data-scroll-target="#implications-for-behaviorism"><span class="header-section-number">7.7.6</span> Implications for Behaviorism</a></li>
  <li><a href="#debate-about-interpretation" id="toc-debate-about-interpretation" class="nav-link" data-scroll-target="#debate-about-interpretation"><span class="header-section-number">7.7.7</span> Debate about interpretation</a></li>
  </ul></li>
  <li><a href="#information-theory-and-beyond" id="toc-information-theory-and-beyond" class="nav-link" data-scroll-target="#information-theory-and-beyond"><span class="header-section-number">7.8</span> Information theory and beyond</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix"><span class="header-section-number">7.9</span> Appendix</a>
  <ul>
  
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/CrumpLab/cognition/edit/main/textbook/C7_Information_Processing.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/CrumpLab/cognition/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">7 Information Processing</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Matthew J. C. Crump </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 1, 2021</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">August 9, 2023</p>
    </div>
  </div>
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    This chapter covers the concepts of processing stages, information, and capacity limitations, which became popular cognitive research topics around the 1950s and 60s.
  </div>
</div>

</header>

<p>This chapter overviews <strong>information processing</strong> as a concept in cognition. As mentioned in chapter 1, Ulrich Neisser defined cognition as “all processes by which the sensory input is transformed, reduced, elaborated, stored, recovered, and used” <span class="citation" data-cites="neisserCognitivePsychology1967">(<a href="#ref-neisserCognitivePsychology1967" role="doc-biblioref">Neisser, 1967</a>)</span>. Neisser’s definition embraces the information processing tradition in cognition. Sensory input contains “information” about the world, and cognition is characterized as the “processing” of that information. We examine the notions of <em>processing stages</em>, <em>information</em>, and <em>capacity limitations</em>, which became popular research topics around the 1950s and 60s.</p>
<p>Some alliterative themes about cognitive research are also introduced. For example, we begin the chapter with the <em>four Rs</em>, referring to the industrial, technological, digital, and “cognitive” revolutions. The first three revolutions introduced new machines that changed the course of human history. Some of these machines also influenced explanations of cognition. In particular, each era inspired mechanistic explanations of cognition that resembled machines of the day. In particular, this chapter discusses the “assembly-line”, “telephone”, and “computer” metaphors of cognition. The first two technologies shaped the concept of information processing so they are given the most attention. The computer metaphor is expanded upon in following chapters.</p>
<section id="four-revolutions-industrial-technological-digital-and-cognitive" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="four-revolutions-industrial-technological-digital-and-cognitive"><span class="header-section-number">7.1</span> Four Revolutions: Industrial, Technological, Digital, and “Cognitive”</h3>
<p>“Revolution” is used to describe periods in history where some innovation led to dramatic changes in society. For example, the <a href="https://en.wikipedia.org/wiki/Industrial_Revolution">industrial revolution</a> in Western Europe and America involved creating large-scale machines, factories and <em>assembly-lines</em> to mechanize the means of production; and, is credited with launching the world into an unprecedented period of sustained growth (e.g., population growth, socio-economic growth). The <a href="https://en.wikipedia.org/wiki/Second_Industrial_Revolution">second industrial revolution</a> (AKA technological revolution), brought the introduction of electricity, <em>telephones for communication</em>, planes, trains, and automobiles for transportation, and new systems for infrastructure like sewage and water supply networks. Eras associated with the introduction of technology are also described in terms of ages, like the <a href="https://en.wikipedia.org/wiki/Machine_Age">machine age</a>, <a href="https://en.wikipedia.org/wiki/Atomic_Age">atomic age</a>, <a href="https://en.wikipedia.org/wiki/Jet_Age">jet age</a>, <a href="https://en.wikipedia.org/wiki/Space_Age">space age</a>. A more recent revolution was the <a href="https://en.wikipedia.org/wiki/Digital_Revolution">digital revolution</a> involving introduction of <em>computer technology</em>, which led into the <a href="https://en.wikipedia.org/wiki/Information_Age">information age</a>. According to wikipedia, the next age could be the <a href="https://en.wikipedia.org/wiki/Imagination_age">imagination age</a> involving immersive virtual reality experiences and an economy primarily driven by “imagination work” <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Psychologists have also used “revolutionary” terms to describe historical periods of research in psychology. For example, the “cognitive revolution” generally refers to the period of experimental psychology following “radical behaviorism”. The figurative imagery implies that “cognitive psychologists” rebelled and overthrew the “behaviorist orthodoxy”. However, the transition between the two schools of thought was very gradual, and several aspects of behaviorism were retained as a part of modern cognition <span class="citation" data-cites="millerCognitiveRevolutionHistorical2003 greenwoodUnderstandingCognitiveRevolution1999 sperryImpactPromiseCognitive1993">(<a href="#ref-greenwoodUnderstandingCognitiveRevolution1999" role="doc-biblioref">Greenwood, 1999</a>; for additional descriptions of the “cognitive revolution” see, <a href="#ref-millerCognitiveRevolutionHistorical2003" role="doc-biblioref">Miller, 2003</a>; <a href="#ref-sperryImpactPromiseCognitive1993" role="doc-biblioref">Sperry, 1993</a>)</span>. In this sense, “revolution” is not a great metaphor for the emergence of cognitive psychology . For example, cognitive psychologist <a href="https://en.wikipedia.org/wiki/George_Mandler">George Mandler</a> notes, “The term ‘revolution’ is probably inappropriate–there were no cataclysmic events, the change occurred slowly in different sub-fields over some 10 to 15 years, there was no identifiable flash-point or leader, and there were no Jacobins” <span class="citation" data-cites="mandlerOriginsCognitiveEvolution23">(<a href="#ref-mandlerOriginsCognitiveEvolution23" role="doc-biblioref">Mandler, 2002</a>)</span>.</p>
<p>Metaphors can shape how people think of one thing in terms of another. The “cognitive revolution” could lead you to imagine a fight against the ruling behaviorists. In this case, the revolution metaphor is not very fitting or insightful. The metaphor does not illuminate important historical aspects of the behaviorism-cognitive transition. Using the metaphor could exaggerate the importance of some historical elements over others by highlighting attention toward elements that fit the metaphor, and misdirecting examination of elements that do not fit the metaphor. However, in other cases a metaphorical relationship can fit very well, be useful for explanation, and even generate insights.</p>
<p>Metaphors are commonly used to describe how cognition works. We will use technological revolutions as metaphors to describe concepts of “information processing” in cognition. The introduction of “information processing” concepts occurred during the transition from behaviorism to cognitivism, and involved mechanistic metaphors from the industrial revolution (the factory assembly line), and the technological revolution (the telephone).</p>
<section id="assembly-line-metaphor-of-cognition" class="level4" data-number="7.1.1">
<h4 data-number="7.1.1" class="anchored" data-anchor-id="assembly-line-metaphor-of-cognition"><span class="header-section-number">7.1.1</span> Assembly-line metaphor of cognition</h4>
<p>A major innovation of the Industrial Revolution was the introduction of machines and factories to automate production. These are physical devices that process and transform raw materials into other refined states, which are further transformed and/or assembled into goods or final products. For example, a factory assembly line for making crayons involves successive processing stages, such as heating wax in a vat, coloring and stirring the liquid, filtering, pouring, and drying, and forming the wax into malleable rolls. Then, the rolls of colored wax are extruded through perforated metal plates (with holes the size of crayons), dried, and placed in machines to wrap and package them.</p>
<p>The assembly line metaphor is also used in cognition and refers to the idea that there are separate stages of processing that transform the “raw materials” of sensation into the “final products” of cognition and action. We examine this metaphor in the section on Donders and his use of mental chronometry to measure assumed stages of processing.</p>
</section>
<section id="telephone-metaphor-of-cognition" class="level4" data-number="7.1.2">
<h4 data-number="7.1.2" class="anchored" data-anchor-id="telephone-metaphor-of-cognition"><span class="header-section-number">7.1.2</span> Telephone metaphor of cognition</h4>
<p>A major innovation of the technological revolution was the introduction of <a href="https://en.wikipedia.org/wiki/Telecommunication">communication technology</a> like the <a href="https://en.wikipedia.org/wiki/Electrical_telegraph">telegraph</a> and <a href="https://en.wikipedia.org/wiki/Telephone">telephone</a>. This technology allowed people to communicate in real-time over long distances, from one device connected by wire to another device. There was a great demand for telephone technology, and the demand was met by creating a massive network of wires to connect phones to each other. Before automation, the telephone network was managed by human operators who worked at central nodes in the network. A caller who picked up a phone to place a call would be immediately connected to a human telephone operator and would ask to be connected to another phone. The operator received the instructions and made the physical connection in the network to connect the incoming call to the destination phone. Around the time of World War II, several psychologists began using elements of the telephone system as a metaphor for cognitive processing. After Donders’ processing stages, we examine the concept of “information” processing which was borrowed largely from telecommunication theory and technology.</p>
</section>
<section id="computer-metaphor-of-cognition" class="level4" data-number="7.1.3">
<h4 data-number="7.1.3" class="anchored" data-anchor-id="computer-metaphor-of-cognition"><span class="header-section-number">7.1.3</span> Computer metaphor of cognition</h4>
<p>A major innovation of the digital revolution was the introduction of computing technology. The rise of computer sciences also occurred in tandem with the growth of modern cognitive psychology. For example, the <a href="https://en.wikipedia.org/wiki/Cognitive_science">“cognitive sciences”</a> are considered an interdisciplinary discipline that includes cognitive psychology, computer science, philosophy, linguistics, neuroscience, and anthropology. Like other transformational technologies, computers have been used as prominent metaphors for cognition. Sometimes cognitive theories are very literal with the metaphor, and cognition is broken down into parts resembling actual physical components of a digital computer. In other cases, cognition is described in terms of more abstract computational processes and algorithms rather than concrete components. Although computers are highly relevant to the “information processing” theme of this chapter, further elaboration on the computational metaphor of cognition will be reserved for upcoming chapters.</p>
<!--

This could go in the modeling chapter?

The behaviorists we just finished reading about successfully squeezed in the study of behavior as a topic of inquiry in its own right. They did this by criticizing mentalism on one side for being unscientific (lacking objectivity and too metaphysical); but, they also had to contend with physiologists on the other side. Physiology was not lacking objectivity: there were liquids, organic tissues, and electrical impulses that could be readily observed. The behaviorists sought to legitimize the objective study of behavior at a macroscopic level, while conceding that physiological processes were obviously causing the behavior at a microscopic level. In distinguishing between behavioral and physiological levels of analysis, behaviorism also made a case against needing to justify itself in terms of lower-level mechanisms. For example, Skinner's system had no mechanisms for any of the behaviors in the system. There were no physical counterparts to reflex strength, or to a reflex reserve. These were not mechanisms, they were just terms he used in his descriptive system. Skinner chose terms that loosely referred to physical entities, but did not extend the metaphors any further. Notably, Skinner argued that the goal was not to explain the mechanisms of behavior, but instead to control and manipulate behavior, and his system was capable of doing that without referring to mechanisms.

-->
</section>
</section>
<section id="the-mechanization-of-cognition" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="the-mechanization-of-cognition"><span class="header-section-number">7.2</span> The mechanization of cognition</h3>
<p>The first metaphorical theme is between cognition and machines from the industrial revolution. From the association chapter, Descartes likened human and animal physiology to a complicated plumbing machine, like the one he saw in the royal gardens. However, Descartes was a dualist who argued that the biological machine of the human body was merely a receptacle for psychic forces. Nevertheless, he inspired physiological psychology and modern neuroscience, which have the aims of explaining cognitive processes in terms of their physical and bio-chemical substrates.</p>
<p>Machines offer a reductive perspective on explanation. The goal of a mechanistic explanation is to explain how the parts of a process produce some complex phenomenon, just like how the connected parts of a machine determine how the machine works.</p>
<p>Machines set a standard for explanation. For example, if the inner workings of the machine of cognition can be “explained” to this standard, then along with other technology, cognition could be manufactured and innovated upon. By analogy, this could include ways to repair, restore, and preserve cognition, as well as create new ways for cognition to work and function. As with the prospects of behavioral engineering, cognitive technologies also raise a host of ethical questions.</p>
<p>Psychology does not always have the goal of achieving a machine-based explanation. For example, some of the behaviorists in the previous chapter deliberately side-stepped mechanistic explanations as a goal. For example, Skinner acknowledged that behavior was ultimately rooted in physical mechanisms, but he argued that behavior itself could also be studied at a macroscopic level, without referring to it’s microscopic mechanisms. He sometimes used terms that loosely referred to mechanisms. For example, a “reflexes” had “strength”, and were emitted after some “impulse” reached a “threshold”. All of these terms could refer to various physical mechanisms; however, Skinner was careful to say that none of them were intended to refer to any real mechanism. They were simply abstract and arbitrary terms in his system that he could have chosen different names for. As a result, many components of Skinner’s theory were not intended to be explained at a lower-level.</p>
<p>Behaviorists had an awkward relationship with mechanistic explanations. They were critical of domains that lacked mechanisms, such as mentalistic and introspectionist psychology; but, also carefully avoided having to describe mechanisms for their domain of psychology. Instead, they were content with terms that had metaphorical connotations of mechanisms, as long as the terms were operationally defined and useful for a descriptive system of behavior.</p>
<p>Stepping past the behaviorist era in to the cognitive one involves a shift in explanatory style from purely descriptive systems to a greater emphasis on mechanistic accounts of cognitive processes. This shift sometimes involves actual physical mechanisms in the brain, body, and environments; but, the shift also involves metaphorical mechanisms.</p>
<p>Metaphorical mechanisms may sound like an oxymoron, but they are very common in science. The general strategy is first to find a simple model system that is “like” a more complicated system under investigation; and then use the simple model as a “metaphor” to describe or generate insights about the more complicated system. The next section discusses mental processing stages and the assembly-line metaphor from the industrial revolution.</p>
</section>
<section id="donders-processing-stages" class="level3 page-columns page-full" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="donders-processing-stages"><span class="header-section-number">7.3</span> Donders’ Processing Stages</h3>
<p>Up to this chapter, we have traced a line through psychology from Galton in 1865 to the period of behaviorism. There are several other starting points we could have chosen. For example, in the same year, 1865, Dutch physiologist F. C. Donders presented behavioral experiments on human reaction times to support the beginnings of a mechanistic theory of cognitive operations <span class="citation" data-cites="kosterPreface1969">(<a href="#ref-kosterPreface1969" role="doc-biblioref">Koster, 1969</a>)</span>.</p>
<p>Donders’ work used an assembly line metaphor for cognition. In an assembly line, raw material is sent from one stage of processing to another, and a product is formed at the end of the whole process. In cognition, sensory input is the raw material that is transformed in successive stages of processing.</p>
<p>In an assembly line, each stage of processing takes some amount of time to perform a particular job. Similarly, in cognition, Donders assumed that there were individual stages of processing (for different cognitive tasks) that took specific amounts of time.</p>
<p>Donders’ major contribution was to develop methods for identifying stages of cognitive processing and estimating how long each of them took to complete. His was work was translated into English with the title, “On the speed of mental processes” <span class="citation" data-cites="dondersSpeedMentalProcesses1868">(<a href="#ref-dondersSpeedMentalProcesses1868" role="doc-biblioref">Donders, 1868–1969</a>)</span>.</p>
<section id="donders-mental-chronometry-and-processing-stages" class="level4" data-number="7.3.1">
<h4 data-number="7.3.1" class="anchored" data-anchor-id="donders-mental-chronometry-and-processing-stages"><span class="header-section-number">7.3.1</span> Donders mental chronometry and processing stages</h4>
<p>Donders was impressed with research on the speed of nerve conduction. Earlier physiologists, such as Johannes Muller, suggested the velocity of nerve conduction may be infinitely fast, or potentially unknowable. But, in 1849, <a href="https://en.wikipedia.org/wiki/Hermann_von_Helmholtz">Hermann von Helmholtz</a> succeeded in measuring nerve transmission speeds in a frog, which fell within the range of 24-36 meters per second.</p>
<p>The empirical observation that nerve conduction was not infinitely fast was very important for Donders. As a physiologist, he assumed that the thinking processes of the mind were controlled by organs in the brain. If the exchange of signals in the nervous system occurred infinitely fast, he assumed that processes of thought might also occur infinitely fast. However, the discovery that nerves had physical properties limiting how fast they conducted signals suggested to Donders that thoughts might have some measurable speed, so he set out to record the speed of thought.</p>
</section>
<section id="physiological-reaction-time" class="level4" data-number="7.3.2">
<h4 data-number="7.3.2" class="anchored" data-anchor-id="physiological-reaction-time"><span class="header-section-number">7.3.2</span> Physiological reaction time</h4>
<p>Donders pointed to the concept of physiological reaction time, which came first from astronomy <span class="citation" data-cites="canalesExitFrogEnter2001">(<a href="#ref-canalesExitFrogEnter2001" role="doc-biblioref">Canales, 2001</a>)</span>. Astronomers made observations about the locations of heavenly bodies in the sky and entered positional coordinates and times of observations about those bodies into their record books. Different observatories around the world would share records. Theories of planetary motion could be tested by comparing the predictions about where planets should be at different times to the recorded data about where planets were observed to be at different times. There were discrepancies between theory and data. Astronomer Adolphe Hirsch wondered how much of the discrepancies were due to human error. Some of the human observers might have slightly faster perceptions than other human observers, and this could introduce error into the entries they made into the astronomical records. Hirsch set out to measure each observer’s “physiological” reaction time.</p>
<p>Imagine you were an astronomer about to look at a star through a telescope. As soon as the light from the star hits your eyeball, how long will it take for you to react to the light?</p>
<p>Donders called this duration “physiological time” in reference to the time it would take for the light stimulus to be transduced through the eye into nervous activity that would eventually lead to a muscle response. Hirsch had already developed methods to precisely measure an individual’s “physiological reaction time”, and he used those measures to account for human error in the astronomical records. Donders had a different use for the method. He wrote:</p>
<blockquote class="blockquote">
<p>“The idea occurred to me to interpose into the process of the physiological time some new components of mental action. If I investigated how much this would lengthen the physiological time, this would, I judged reveal the time required for the interposed term.”</p>
</blockquote>
</section>
<section id="donders-mental-reaction-times" class="level4 page-columns page-full" data-number="7.3.3">
<h4 data-number="7.3.3" class="anchored" data-anchor-id="donders-mental-reaction-times"><span class="header-section-number">7.3.3</span> Donders mental reaction times</h4>
<p>Donders conducted human reaction time experiments with tactile, visual and auditory stimuli using a similar method. A subject was presented with a stimulus and they responded to the stimulus as quickly and accurately as possible. Donders measured reaction time, which was defined as the amount of time between the onset of the stimulus and a subsequent response.</p>
<p>One question was whether different sense organs had different physiological reaction times. Additionally, Donders was interested in the amount of “mental” time it might take to perform increasingly complex tasks before making a response to a stimulus. Donders measured reaction times across three similar tasks that varied in complexity.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-7Donders_simple" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_simple.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;1: A simple reaction time task involves detecting any stimulus as fast as possible.</figcaption>
</figure>
</div>
</div></div></div>
<p><a href="#fig-7Donders_simple">Figure&nbsp;1</a> the <em>simple reaction time task</em>. This task measures the duration a person takes to recognize that a stimulus, such as a light, has been presented. Participants are instructed to wait for the stimulus and respond as swiftly as possible, immediately upon detection. Donders considered performance in this task to measure “physiological reaction time”.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-7Donders_GONoGo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_GoNoGo.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2: A Go-NoGo task involves detecting a specific ‘go’ stimulus (aqua light) as fast as possible, and witholding responses to ‘no-go’ stimuli (red light).</figcaption>
</figure>
</div>
</div></div></div>
<p>Donders had subjects perform a slightly more complicated task shown in <a href="#fig-7Donders_GONoGo">Figure&nbsp;2</a>, now referred to as the <em>GO-NO GO</em> task. In this task, participants are shown a stimulus but are told to respond only when they are presented with the targeted ‘go’ stimulus. For instance, the stimulus could be either blue or red; participants would be required to respond only when the stimulus is blue, a response known as a ‘Go’ response. If the stimulus is red, participants should not respond, classified as a ‘No Go’ trial.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-7Donders_Choice" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_Choice.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;3: A choice reaction time task presents one of many stimuli, each requiring their own specific response.</figcaption>
</figure>
</div>
</div></div></div>
<p>Last, <a href="#fig-7Donders_Choice">Figure&nbsp;3</a> depicts a more complicated <em>alternative forced-choice task</em>. Here, participants are shown one of many stimuli and are required to respond to each with a specific response. For example, a subject could be asked to respond to a blue stimulus by pressing a left button, and to respond to a red stimulus by pressing a right button. This would be called a 2-AFC (two-alternative forced choice) task.</p>
</section>
<section id="donders-subtractive-stage-logic" class="level4" data-number="7.3.4">
<h4 data-number="7.3.4" class="anchored" data-anchor-id="donders-subtractive-stage-logic"><span class="header-section-number">7.3.4</span> Donders subtractive stage logic</h4>
<p>Donders used the subtraction of reaction times between tasks to estimate the speed of specific mental operations. He assumed that mental operations occurred in successive stages, just like an assembly line (shown in <a href="#fig-7Donders_Assembly">Figure&nbsp;4</a>), and that each stage took an average amount time.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Donders_Assembly" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_Assembly.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;4: A cartoon depiction of a cognitive assembly line. A stimulus enters the assembly line as raw ingredients for cognition. The stimulus is transformed across cognitive processing stages to produce a response.</figcaption>
</figure>
</div>
</div>
</div>
<p>According to Donders, the amount of time taken to respond to a stimulus should depend on how many processing stages are required for the task at hand. Simpler tasks have fewer processing stages and can be performed more quickly than more complicated tasks that require additional processing stages. These ideas are illustrated in <a href="#fig-7Donders_task_stages">Figure&nbsp;5</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Donders_task_stages" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_task_stages.jpg" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Illustration showing how reaction times should increase as a function of task complexity.</figcaption>
</figure>
</div>
</div>
</div>
<p>The fastest reaction time should be the physiological time found in the simple reaction time task. Reaction times should increase in length when additional mental processing is required before a response. For example, the GO-NO GO task should produce a longer reaction time than the simple reaction time task. This is because the task requires an additional mental operation of stimulus identification. In the GO-NO GO task, the stimulus must be identified as the target stimulus before a response is made. <a href="#fig-7Donders_id_time">Figure&nbsp;6</a> shows how Donders used subtractive logic to infer the time taken to complete mental operations like stimulus identification.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Donders_id_time" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_id_time.jpg" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Employing subtractive logic to estimate stimulus identification time.</figcaption>
</figure>
</div>
</div>
</div>
<p>For example, if it took 170 milliseconds to make a response in the GO-NO GO task, and 150 milliseconds to make a response in the simple reaction time task, then Donders took the difference of 20 milliseconds (by subtraction 170-150 = 20) to index stimulus identification time.</p>
<p>This subtraction logic can be applied to infer the times of subsequent stages of processing. For example, reaction times in the choice task (2AFC) are longer than in a GO-NO GO task. Following Donders logic, a 2AFC task involves yet another mental operation: <em>response selection</em>. For example, in a choice task a stimulus must be identified, and then the correct response (e.g., right or left) must be selected before the final response is made.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Donders_RT_time" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Donders_RT_time.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Employing subtractive logic to estimate response selection time.</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-7Donders_RT_time">Figure&nbsp;7</a> shows that response selection time could be measured by subtracting the reaction time in the 2AFC task, from the reaction time in the GO-NO GO task.</p>
</section>
</section>
<section id="beyond-donders" class="level3 page-columns page-full" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="beyond-donders"><span class="header-section-number">7.4</span> Beyond Donders</h3>
<p>The next sections review developments of Donders’ subtraction logic, research on human reaction times, and the concept of processing stages since his proposals in 1865.</p>
<section id="subtractive-logic" class="level4" data-number="7.4.1">
<h4 data-number="7.4.1" class="anchored" data-anchor-id="subtractive-logic"><span class="header-section-number">7.4.1</span> Subtractive logic</h4>
<p>Donders’ subtractive logic is a nifty idea, but it doesn’t necessarily produce correct inferences about cognitive operations. One issue is the distinction between <em>serial and parallel</em> processing. If the intervening mental operations between a stimulus and response truly function like a simple <em>serial</em> assembly-line, then the subtractive logic can work very well. In this case, the mental operations take place one after the other in a series. By subtracting reaction times from tasks involving one less mental operation, the total reaction time for a task involving two or more mental operations (which occur in serial stages) can be deduced.</p>
<p>However, a problem arises for subtractive logic when operations can be done simultaneously or <em>in parallel</em>. In this scenario, different processing stages might occur concurrently, and measuring additional time spent on a mental task may not necessarily reflect distinct processing stages. Another problem occurs if the stages are unpredictable, and therefore it does not make sense to subtract times that are not constants.</p>
<p>To address some of the inferential problems with subtractive logic for making inferences about putative processing stages, Saul Sternberg introduced another method called <em>additive-factors logic</em> <span class="citation" data-cites="sternbergDiscoveryProcessingStages1969">(<a href="#ref-sternbergDiscoveryProcessingStages1969" role="doc-biblioref">Sternberg, 1969</a>)</span>. Subtractive logic also became commonly used in neuroimaging research to find areas of brain activity uniquely correlated with specific task demands <span class="citation" data-cites="alexanderDondersDeadCortical2015">(<a href="#ref-alexanderDondersDeadCortical2015" role="doc-biblioref">Alexander et al., 2015</a>)</span>.</p>
</section>
<section id="reaction-time-research" class="level4" data-number="7.4.2">
<h4 data-number="7.4.2" class="anchored" data-anchor-id="reaction-time-research"><span class="header-section-number">7.4.2</span> Reaction time research</h4>
<p>The measurement of reaction times to make inferences about cognitive processes became widespread and remains a very common measurement tool in cognition. For example, we already discussed Cattell’s associative reaction time research from 1886, which was clearly inspired by Donders. In 1890, Jastrow reviewed the promising uses of reaction time methods in psychology in a book called, <em>Time Relations of Mental Phenomena</em> <span class="citation" data-cites="jastrowTimerelationsMentalPhenomena1890">(<a href="#ref-jastrowTimerelationsMentalPhenomena1890" role="doc-biblioref">Jastrow, 1890</a>)</span>. Reaction times would be used throughout every decade to study cognitive processes in humans, even throughout the behaviorist period. We will continue to discuss reaction time research throughout this chapter and others.</p>
</section>
<section id="processing-stages" class="level4 page-columns page-full" data-number="7.4.3">
<h4 data-number="7.4.3" class="anchored" data-anchor-id="processing-stages"><span class="header-section-number">7.4.3</span> Processing Stages</h4>
<p>Donders’ concept of mental processing stages disappeared for a while during the behaviorist era. Although some behaviorists (like Tolman and Hull) were willing to speculate about intervening processes between a stimulus and response, other forms of behaviorism were not interested in whatever mental operations might be taking place. As a result, the possibility that there was a mental processing stage for stimulus identification, response selection, or other mental operations was not of scientific interest.</p>
<p>The concept of processing stages came back in different ways, and we will see more examples in the chapters on memory, attention, and computational modeling. As a historical side note, Donders’ ideas regained popularity in the field of cognition approximately a century after his original publication. His centenary was commemorated at the second <a href="http://www.attentionandperformance.org">Attention and Performance</a> conference in 1968, which took place in the Netherlands. This ongoing conference series features invited speakers and continues to publish books that compile the papers presented at each conference. During the early conference years, numerous articles established the importance of processing stages in cognition. One notable example to briefly mention is the <a href="https://en.wikipedia.org/wiki/Psychological_refractory_period">psychological refractory period</a>.</p>
<section id="prp-psychological-refractory-period" class="level5 page-columns page-full" data-number="7.4.3.1">
<h5 data-number="7.4.3.1" class="anchored" data-anchor-id="prp-psychological-refractory-period"><span class="header-section-number">7.4.3.1</span> PRP: Psychological Refractory Period</h5>
<p>British psychologist A. T. Welford drew attention to the “Psychological Refractory Period” in a 1952 paper <span class="citation" data-cites="welfordPsychologicalRefractoryPeriod1952 welfordEvidenceSinglechannelDecision1959">(<a href="#ref-welfordPsychologicalRefractoryPeriod1952" role="doc-biblioref">Welford, 1952</a>; see also <a href="#ref-welfordEvidenceSinglechannelDecision1959" role="doc-biblioref">Welford, 1959</a>)</span>. In the preceding war years, basic research on human reaction times was conducted for the war effort. For example, the ability to make a quick reaction could be important for a pilot. As a result, empirical studies were conducted to identify useful information about human reaction times. Useful information could include how to make reactions more efficient, or to discover limitations in reaction times that could be addressed. For example, if people are pressing buttons or flicking switches in a cockpit, how should the cockpit be designed to improve the speed and accuracy of the responses?</p>
<p>Welford observed the psychological refractory period from existing reaction time research and discussed possible explanations for the finding in terms of processing stages. The basic PRP effect was that responding to one stimulus can sometimes delay a response to a second stimulus, especially if they are presented quickly, one after the other.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-7PRP_effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/PRP_effect.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Illustration of the psychological refractory period. A response to a second stimulus is slowed when the second stimulus is presented quickly after the first.</figcaption>
</figure>
</div>
</div></div></div>
<p>The conditions to observe the PRP effect are depicted in <a href="#fig-7PRP_effect">Figure&nbsp;8</a>. In the task, participants are presented with stimuli (S) one after the other, and asked to respond (R) as fast as possible to each of them, for example, by pressing a button.</p>
<p>The long-delay condition highlights an important difference between two successive stimuli. In this case, the second stimulus (<span class="math inline">\(S_2\)</span>) appears after a long temporal delay following the response (<span class="math inline">\(R_1\)</span>) to the first stimulus <span class="math inline">\(S_1\)</span>. When this delay is long enough, the average reaction time to the first and second stimulus are generally the same (<span class="math inline">\(RT_1 = RT_2\)</span>). However, when the delay is shortened, the PRP effect is observed: the average reaction time to the second stimulus becomes longer than the average reaction time to the first stimulus (<span class="math inline">\(RT_1 &lt; RT_2\)</span>). This lengthening of the second response time is an example of the so-called “psychological refractory period”.</p>
<p>The PRP effect is a reliable finding in reaction time research, and since Welford, hundreds of papers have been published on the phenomenon. Note that as we move forward, we will discuss many findings like the PRP effect that have inspired a large number of research papers. The term “paradigm” is commonly used to refer to collections of research activity that are relatively focused on specific phenomena or tasks. For example, we could refer to the maze-running research discussed last chapter as a paradigm using the task of maze-running. The PRP paradigm is full of papers testing explanations of the PRP phenomenon. My purpose in bringing up the PRP literature is not for us to become experts in this one phenomenon; instead, the PRP effect provides a clear example of the metaphorical use of processing stages to provide a quasi-mechanical explanation of the phenomena.</p>
<p>So, why does the PRP effect occur? Why is the second response time lengthened if the second stimulus is presented shortly after a previous response? In 1959, Welford described five different theoretical accounts. One possibility was a physical explanation in terms of limitations of signaling among nerve fibers. Another theory had to do with preparedness or expectancy, perhaps the shorter duration caused people to be more surprised, and it was the surprise that lengthened the second response. The fifth hypothesis involved a <em>central mechanism with a single channel of limited capacity</em>, and was summarized as follows <span class="citation" data-cites="welfordEvidenceSinglechannelDecision1959">(<a href="#ref-welfordEvidenceSinglechannelDecision1959" role="doc-biblioref">Welford, 1959</a>)</span>:</p>
<blockquote class="blockquote">
<p>“In its bare essentials this theory assumes, firstly, a number of sensory input mechanisms each capable of receiving data and storing it for a limited period so that, for example, a short series of signals can be received as a unit. Secondly, it assumes a number of effector mechanisms containing both central and peripheral elements and capable of carrying out a series of actions such as the pressing and release of a key or a series of taps (Vince, 1949) as a single unit. Thirdly, between these two it postulates a single-channel decision mechanism. This is regarded as being of limited capacity in the sense that it takes a finite time to process information and can thus only deal with a limited amount of information in a given time.”</p>
</blockquote>
<p>The mechanism being proposed is like the processing stages of a serial assembly line. A stimulus is first processed in the perceptual stage and then moved to the next central processing stage. The central processing stage produces the decision to respond to the stimulus. This decision is sent to a response production stage, and the response is made. Furthermore, the central processing stage is claimed to be “capacity limited”. For example, what if it could only deal with one decision at a time? This would create a bottleneck in performance. If a second stimulus entered the central stage, it would have to wait in line until the decision to respond to the first stimulus was sent to the next stage.</p>
<p>Welford and others <span class="citation" data-cites="craikTheoryHumanOperator1948 hickDiscontinuousFunctioningHuman1948 davisHumanOperatorSingle1957 fraissePeriodeRefractairePsychologique1957 broadbentMechanicalModelHuman1957 broadbentPerceptionCommunication1958">(<a href="#ref-broadbentMechanicalModelHuman1957" role="doc-biblioref">Broadbent, 1957</a>, <a href="#ref-broadbentPerceptionCommunication1958" role="doc-biblioref">1958</a>; <a href="#ref-craikTheoryHumanOperator1948" role="doc-biblioref">Craik, 1948</a>; <a href="#ref-davisHumanOperatorSingle1957" role="doc-biblioref">Davis, 1957</a>; <a href="#ref-fraissePeriodeRefractairePsychologique1957" role="doc-biblioref">Fraisse, 1957</a>; <a href="#ref-hickDiscontinuousFunctioningHuman1948" role="doc-biblioref">Hick, 1948</a>)</span> were part of a new wave of researchers applying concepts from telecommunications science and technology to human cognition. For example, the idea of a single-channel decision mechanism processing information in a capacity-limited manner was motivated by telephone technology. The second half of this chapter expands on this metaphor, detailing how it influenced the concept of information processing in cognition.</p>
<!-- general issues with the metaphor 

- appeals to mechanism
- assembly lines do things in order, and each stage takes time...the subtractive logic does apply to orderly assembly lines
- what if cognition is not so orderly?
- Does the interpretation of what a task might logically involve force a conclusion that about stages of processing? Do stages of processing actually exist, or are they convenient metaphors? 

-->
</section>
</section>
</section>
<section id="cybernetics-and-the-macy-conferences" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="cybernetics-and-the-macy-conferences"><span class="header-section-number">7.5</span> Cybernetics and the Macy Conferences</h3>
<p>We’ve been jumping around a little bit in historical time. Last chapter ended with Skinner circa 1938, and this chapter went back to Donders circa 1868, and then forward in time to the Attention and Performance conference of 1968. That leaves roughly a 30 year gap between 1938 and 1968 that we didn’t talk much about. This was a dense historical period that informed the cognitive sciences. There was a world war, the invention of nuclear weapons, the invention of digital computers, and the gradual transition in American psychology from behaviorism to cognitivism. This time period also contained the Macy Conferences and Cybernetics, which provide useful perspectives on the transition into the cognitive era.</p>
<p><a href="https://en.wikipedia.org/wiki/Macy_conferences">The Macy conferences</a> were held (160 over 19 years) in New York between 1941 and 1960. These conferences were sponsored by the <a href="https://en.wikipedia.org/wiki/Josiah_Macy_Jr._Foundation">Josiah Macy J. Foundation</a> and involved attendees from a variety of academic backgrounds to encourage interdisciplinary interactions and exchange of ideas. For example, many attendees were academics from the disparate fields of psychology, philosophy, anthropology, physiology, linguistics, genetics, and math and computer science (and others). And, rather than discussing specialty topics of little relevance to other fields, they were interested in discussing potential overlaps and bigger issues requiring sharing and integration of methods between fields.</p>
<p>Many of these exchanges revolved around a movement called <a href="https://en.wikipedia.org/wiki/Cybernetics">cybernetics</a>. Cybernetics still exists as a field, but is also recognized as the beginning of what is now called the cognitive sciences. Cybernetics played a transitional role in American psychology by bridging elements of behaviorism with later cognitivism.</p>
<p><a href="https://en.wikipedia.org/wiki/Norbert_Wiener">Norbert Weiner</a>, acclaimed as the father of cybernetics, initiated this trans-disciplinary approach that focuses on the “control and communication in both the animal and the machine” <span class="citation" data-cites="wienerCyberneticsControlCommunication1948">(<a href="#ref-wienerCyberneticsControlCommunication1948" role="doc-biblioref">Wiener, 1948</a>)</span>. This concept was further expanded by mathematician <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">A. K. Kolmogorov</a>, defined cybernetics as “concerned with the study of systems of any nature which are capable of receiving, storing and processing information so as to use it for control.” <span class="citation" data-cites="umplebyDefinitionsCybernetics2008">(see also many more definitions of cybernetics in <a href="#ref-umplebyDefinitionsCybernetics2008" role="doc-biblioref">Umpleby, 2008</a>)</span>. In Kolmogorov’s sense, Skinner’s behaviorism was an example of cybernetics with some important missing elements– he was studying principles of behavioral control in an animal system, but he was not too concerned about internal processes that might be responsible for receiving, storing, and processing information. Cybernetics had a much broader vision of control, which was to understand the principles of any type system that appeared to regulate itself, including humans, animals, and even machines. The hope was that the principles would be somewhat generalizable across systems and allow insights from one area to foster innovations in another. For example, if principles of cognitive processing in humans could be better understood, then perhaps machines could be made to operate by those principles, which could lead to artificial forms of intelligence. Thus, sharing findings and methods between domains was encouraged because of the potential for new and unexpected insights.</p>
<p>In other words, cybernetics was okay with psychologists exploring mechanistic metaphors for cognitive processing. For example, from a cybernetics perspective, a psychologist might benefit from learning something about telecommunications technology because those methods could provide a model system for understanding something about how people communicate. Whatever insights were extracted could, in turn, be useful for understanding communication in other systems, like animals, machines, or other networks using a communication concept. Indeed, major advancements in telecommunication technology were being discussed at the Macy conferences, and the attending psychologists were very quick to apply those advancements to an emerging cognitive psychology. One of the major advancements was Claude Shannon’s “Information Theory,” which is described first, followed by two ways that it was applied as a metaphor for cognition.</p>
</section>
<section id="shannons-information-theory" class="level3" data-number="7.6">
<h3 data-number="7.6" class="anchored" data-anchor-id="shannons-information-theory"><span class="header-section-number">7.6</span> Shannon’s Information Theory</h3>
<p>There were numerous attendees at the cybernetics conferences whose contributions to the cybernetics movement were also foundational for the cognitive sciences. Most relevant to this chapter was the American mathematician <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> (1916 – 2001). His 1937, master’s degree (MIT) was titled “A Symbolic Analysis of Relay and Switching Circuits” <span class="citation" data-cites="shannonSymbolicAnalysisRelay1938">(<a href="#ref-shannonSymbolicAnalysisRelay1938" role="doc-biblioref">Shannon, 1938</a>)</span>. This was a theoretical paper about the math and logic behind the telephone exchange networks of the day. The exchanges had been automated so that they no longer required a human operator to connect one phone to another, and Shannon’s analysis suggested more efficient designs for switches making the connections. The very same math would later be fundamental for the design of circuits in digital computers. In 1940 Shannon completed his Ph.D.&nbsp;titled, “An Algebra for Theoretical Genetics” <span class="citation" data-cites="shannonAlgebraTheoreticalGenetics1940">(<a href="#ref-shannonAlgebraTheoreticalGenetics1940" role="doc-biblioref">Shannon, 1940</a>)</span> based on his work at the Eugenic Record Office at Cold Springs Harbor Laboratory. During world war II, he worked at Bell lab’s on “A mathematical theory of cryptography”, which involved methods to send and receive messages on communication lines that might have many other listeners besides the intended recipient. Then, in 1948-49 he published what is now called “Information theory” in a book called “The Mathematical theory of communication” <span class="citation" data-cites="shannonMathematicalTheoryCommunication1949">(<a href="#ref-shannonMathematicalTheoryCommunication1949" role="doc-biblioref">Shannon &amp; Weaver, 1949</a>)</span>.</p>
<p>Information Theory was not developed as a theory for cognition or psychology. It offers a way to mathematically describe general elements of communication systems and has found useful applications in many domains, including psychology. We will focus on two ideas from information theory that became popular in early cognitive research. These are the concept of an information channel and the idea that information can be measured and quantified using Shannon’s formula for entropy (<span class="math inline">\(H\)</span>).</p>
<section id="information-channels" class="level4" data-number="7.6.1">
<h4 data-number="7.6.1" class="anchored" data-anchor-id="information-channels"><span class="header-section-number">7.6.1</span> Information channels</h4>
<p>An information channel has three major parts– a sender, a channel, and a receiver–and two big questions: how much information was sent? And, how much was received?</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Shannon_info_channel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Shannon_info_channel.jpg" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Illustration of a toy telephone as a metaphor for an information channel.</figcaption>
</figure>
</div>
</div>
</div>
<p>The toy telephone system made out of tin cans and string in <a href="#fig-7Shannon_info_channel">Figure&nbsp;9</a> is a simple information channel. In this system, one person speaks into a tin can on one end. There is a hole in the bottom of the can, and a knot is securely blocking the hole so that the remaining length of string can be unrolled for a distance and connected to another tin can. The person on the other end is holding the second can up to their ears and listening to the message from the sender.</p>
<p>The speaker’s vocal cords push air through their mouth, creating airwaves in the range of audible frequencies. These waves project into the can and, through a process of physical resonance, transfer the wave pattern in the air to the can. As a result, the can begins to wave with similar frequencies. The can is connected to a string, which also starts waving in the same way, carrying the wave across to the other can. Now, the other can starts to wave too, shaping the air inside it and causing new airwaves to be emitted. These airwaves then travel through the air and into the ear canal of the person listening on the other end. The outer, middle, and inner ear then begin vibrating as well, transducing the mechanical waves into nerve impulses. Eventually, the receiver listens to the message and maybe says something back, repeating the process.</p>
<p>Acts of cognition are important bookends to this story. From the beginning, we can ask how it is that someone can speak a message at all (or pick up a can or make a toy telephone), and at the end, we can ask how someone hears and understands a message and decides to respond to it or not. However, for our immediate purposes, we will not focus on those psychological aspects but instead return to some of the questions about the information channel, which is the medium through which the message passes.</p>
<p>An information channel is a general concept with an important property called capacity. An information channel could be a tin-can telephone with a real string connecting two devices, or it could be a wireless cell phone connected through more advanced technology involving very high frequency waves. Both kinds of phones have a limited ability to send signals; this is called the channel capacity. For example, you can hook up one can to another and hear someone speak on the other end. In loose terms, we could say a string has the capacity to support one message. However, if you hook up more cans to the same string and allow many people to talk at once, the quality of the signal at the receiving end will become increasingly worse. In this sense, the string has a limited amount of capacity to transmit a signal.</p>
<p>Questions about information capacity were fundamental for improving telecommunications technology. For example, what was the information capacity of a physical telephone line? How many calls could it support? What happens when the capacity is exceeded? How could the systems be improved to increase the capacity and support more calls? Could the lines support other kinds of signals? If so, how much? What other kinds of signals?</p>
</section>
<section id="measuring-information-h" class="level4" data-number="7.6.2">
<h4 data-number="7.6.2" class="anchored" data-anchor-id="measuring-information-h"><span class="header-section-number">7.6.2</span> Measuring Information: H</h4>
<p>One of Shannon’s mathematical contributions was to propose a definition for quantifying the amount of information. His formula defines information in terms of entropy, or the amount of uncertainty in a system of messages, and is called Shannon’s <span class="math inline">\(H\)</span>:</p>
<p><span class="math inline">\(H(X) = -1*\sum_\text{i=1}^n P(x_i) * log_2 P(x_i)\)</span></p>
<p>A formula for quantifying information meant that capacity limitations of information channels could be measured and assigned a value indicating how much information they could carry. Additionally, the content of signals could be analyzed and described in terms of how much information was contained in the signal. Finally, when signals are sent across an information channel and received at the other end, sometimes parts of the signal are lost during transmission. The amount of information lost could now be described and accounted for with Shannons’s <span class="math inline">\(H\)</span>.</p>
<p>Shannon’s formula defines information in terms of the predictability of a sequence of messages. For example, imagine a long book with 1,000 pages, but the only word printed in it is the letter “A” many times. Shannon’s formula would say this book transmits zero information because the message is 100% predictable. In Shannon’s theory, communication of information does not occur if the receiver already knows the content of the message.</p>
<p>By contrast, according to Shannon’s definition, the amount of information in a message increases as it becomes more unpredictable. For example, a short book could contain many sentences with words in new combinations that you had never encountered before. When you read the book, you find almost every new statement to be unexpected and surprising. According to Shannon, this kind of book contains much more information than the book of As, which has a long message that is entirely expected.</p>
<p>Shannon takes the definition of information to an oddly extreme place. By definition more random messages have more information, and the ultimate message carrying the most possible information is total randomness <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>One way to conceptualize this is to think of total randomness as containing all possible messages in a system. For example, consider Borel’s (1913) infinite monkey theorem <span class="citation" data-cites="borelMecaniqueStatiqueIrreversibilite1913">(<a href="#ref-borelMecaniqueStatiqueIrreversibilite1913" role="doc-biblioref">Borel, 1913</a>)</span>, which says that a room full of monkeys typing letters on a keyboard for infinity will eventually produce any text, even the works of Shakespeare. So, even though most of the books written by the typing monkeys will be totally incoherent, they are producing all of the possible ways to print letters in books. Therefore, they are writing all of the books that make sense, and all of the ones that don’t <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>This last example is a warning to not equate Shannon’s definition of information with meaningfulness. For example, according to Shannnon, the above books that contain meaningful text to humans would actually convey less information than completely random books. More simply, Shannon’s <span class="math inline">\(H\)</span> is a single number to describe the amount of randomness in a system. If the system was a coin that could transmit one of two messages–heads or tails–then, Shannon’s H is a measure of how fair or biased the coin is. A fair coin is completely random, and transmits the maximal amount of information. A biased coin comes up heads or tails more often. As a result it is more predictable, and transmits less information.</p>
</section>
<section id="computing-h" class="level4" data-number="7.6.3">
<h4 data-number="7.6.3" class="anchored" data-anchor-id="computing-h"><span class="header-section-number">7.6.3</span> Computing H</h4>
<p>Let’s use the coin flipping example to compute <span class="math inline">\(H\)</span>, or the amount of information according to Shannon’s formula. Here is the formula again:</p>
<p><span class="math inline">\(H(X) = -1*\sum_\text{i=1}^n P(x_i) * log_2 P(x_i)\)</span></p>
<p>The capital <span class="math inline">\(X\)</span> refers to the set of discrete events that can occur in a series of messages.</p>
<p>In a coin toss there are two events, heads or tails. The term <span class="math inline">\(P(x_i)\)</span> refers to the probability that each event occurs, and <span class="math inline">\(log_2 P(x_i)\)</span> refers to taking the logarithm base 2 of that same probability.</p>
<p>To state what the formula says in a sentence: multiply (<span class="math inline">\(*\)</span>) the probability of an event (<span class="math inline">\(P(x_i)\)</span>) by its base two logarithm and find the product, do the same for all events (from i= 1 to n, the number of events), add up all of the products (<span class="math inline">\(\sum_\text{i=1}^n\)</span>), and then multiply the final sum by -1.</p>
<p>The table below shows the calculation of <span class="math inline">\(H\)</span> for a fair coin. A fair coin has two possible events, heads or tails, and each event has the same .5 probability of occurring.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">Events</th>
<th style="text-align: left;"><span class="math inline">\(i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(log_2 P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)*log_2 P(x_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Heads</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">-1</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tails</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">-1</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">-1</td>
</tr>
<tr class="even">
<td style="text-align: left;">H</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(-1*\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>To walk through the table, in the first row we have the calculations for heads. The probability of heads is .5.</p>
<p><span class="math inline">\(P(x_i) = .5\)</span></p>
<p>The logarithm base 2 of .5 is -1.</p>
<p><span class="math inline">\(log_2 P(x_i = .5) = -1\)</span></p>
<p>Multiplying the two together gives -.5.</p>
<p><span class="math inline">\(P(x_i)*log_2 P(x_i) = .5 * -1 = -.5\)</span></p>
<p>In row two, we see the same calculations because the probability of tails is also .5. The last steps of the formula involve summing up the products in the last column.</p>
<p><span class="math inline">\(\sum=-.5 + -.5 = -1\)</span></p>
<p>Finally to convert to a positive number, the sum is multiplied by a -1, (-1 * -1 = 1). So, for a fair coin, <span class="math inline">\(H = 1\)</span>.</p>
</section>
<section id="bits-of-information" class="level4" data-number="7.6.4">
<h4 data-number="7.6.4" class="anchored" data-anchor-id="bits-of-information"><span class="header-section-number">7.6.4</span> Bits of information</h4>
<p>Shannon’s formula uses a base two logarithm that outputs a number in the unit of <a href="https://en.wikipedia.org/wiki/Bit">bits</a>. Bits are also the building blocks of modern digital computers. One bit represents a single binary operator that can be in one of two possible states. For example, a coin could represent a bit. If we stipulated that a coin could only land heads or tails, then it would function the same as a binary operator with two possible states. Another binary operator is a logic statement that can either be TRUE or FALSE. It is also possible to express numbers using binary symbols.</p>
<p>Bits also provide a measure the total number of discrete events in a system of messages. Let’s see how.</p>
<p>A single bit has two states, 0 or 1. So, a single bit can represent two unique states, like heads or tails.</p>
<p>How many states can two bits represent? This would involve counting all of the unique ways of combining the states from two bits. All of the four possibilities are: 00, 01, 10, and 11. <a href="#fig-7Shannon_bits">Figure&nbsp;10</a> shows the relationship between number of bits, and the number of unique events that can be represented by combining bits together.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Shannon_bits" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Shannon_bits.jpg" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Illustration of how bits translate to representing indvidudal events.</figcaption>
</figure>
</div>
</div>
</div>
<p>The relationship between number of bits and number of unique events that they can code is defined by raising 2 to the number of <span class="math inline">\(Bits\)</span>:</p>
<p><span class="math inline">\(2^\text{Bits} = \text{number of events}\)</span></p>
<p>The figure shows some examples of computing the number of unique combinations that can be coded with up to three bits.</p>
</section>
<section id="h-bits-predictability-and-information" class="level4" data-number="7.6.5">
<h4 data-number="7.6.5" class="anchored" data-anchor-id="h-bits-predictability-and-information"><span class="header-section-number">7.6.5</span> H, Bits, predictability and Information</h4>
<p>Let’s put these concepts together to discuss a set of messages in Shannon’s communication system. Remember, a sender sends a message to a receiver across an information channel. In this system, important questions are how much information is in the message? How much capacity to transmit information does the channel have? And, how much information is received or lost in transmission?</p>
<p>Shannon’s formula provides a way to calculate answers to these questions; however, in order to do the calculations the message needs to be converted into discrete events so that it can be measured in terms of bits.</p>
<p>Consider a simple communication system where a sender can only send one of four events: A, B, C, or D. How many bits are needed to represent these four events? From the figure above we can see that the answer is four bits.</p>
<p>If a sender is communicating only discrete events like As, Bs, Cs, and Ds, then Shannon’s formula provides a way to measure the amount of uncertainty in the message.</p>
<p>The most uncertainty occurs when the message is completely random. By definition, this means that the sender randomly chooses to send As, Bs, Cs, and Ds with equal probability. This is like a four-sided coin flip (if that was possible). Each of the probabilities is .25, or 1/4. In this situation, the receiver has no way of predicting which event will occur as they receive the message. The events are maximally uncertain.</p>
<p>Watch what happens when we compute <span class="math inline">\(H\)</span> using Shannon’s formula. The answer is <span class="math inline">\(H = 2\)</span>, which is the same as the number of bits needed to represent each of the four events:</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">Events</th>
<th style="text-align: left;"><span class="math inline">\(i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(log_2 P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)*log_2 P(x_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">-2</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">-2</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">-2</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr class="even">
<td style="text-align: left;">D</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">-2</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">-2</td>
</tr>
<tr class="even">
<td style="text-align: left;">H</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(-1*\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">2</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We have just seen that when a communication involves a maximally unpredictable set of events, Shannon’s formula for <span class="math inline">\(H\)</span> returns the number of bits needed to represent each of the unique events in the message. In other words, the number of bits represents an upper bound on the amount of information in a message. In this case, it represents maximal uncertainty when the events occur with equal probability.</p>
<p>What if the events do not occur with equal probability? This would mean that some of the events are more likely than others. In Shannon’s system, whenever some events are more likely than others something special happens at the receiving end of the message. The receiver is now able to predict some of the message. For example, if the message was 70% As, 10% Bs, 10% Cs, and 10% Ds, the receiver would be able to predict that each event has a high probability of being an A, and a low probability of being a B, C, or D. Let’s enter this situation into the formula for H and see what happens:</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">Events</th>
<th style="text-align: left;"><span class="math inline">\(i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(log_2 P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)*log_2 P(x_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.7</td>
<td style="text-align: left;">-0.514573172829758</td>
<td style="text-align: left;">-0.360201220980831</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">-3.32192809488736</td>
<td style="text-align: left;">-0.332192809488736</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">-3.32192809488736</td>
<td style="text-align: left;">-0.332192809488736</td>
</tr>
<tr class="even">
<td style="text-align: left;">D</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">-3.32192809488736</td>
<td style="text-align: left;">-0.332192809488736</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">-1.35677964944704</td>
</tr>
<tr class="even">
<td style="text-align: left;">H</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(-1*\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">1.35677964944704</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In this case, <span class="math inline">\(H\)</span> is computed as 1.35, which means that events in the message require less than 2 bits. There are still four events, but one of them is more predictable then the others. If we made one of the events even more predictable (e.g., like A = .97), then the amount of bits needed would decrease and get closer to zero.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">Events</th>
<th style="text-align: left;"><span class="math inline">\(i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(log_2 P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)*log_2 P(x_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">-0.0439433475875971</td>
<td style="text-align: left;">-0.0426250471599691</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">-6.64385618977472</td>
<td style="text-align: left;">-0.0664385618977472</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">-6.64385618977472</td>
<td style="text-align: left;">-0.0664385618977472</td>
</tr>
<tr class="even">
<td style="text-align: left;">D</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">-6.64385618977472</td>
<td style="text-align: left;">-0.0664385618977472</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">-0.241940732853211</td>
</tr>
<tr class="even">
<td style="text-align: left;">H</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(-1*\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">0.241940732853211</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>If one of the event occurs 100% of the time, and the others occur 0% of the time, then <span class="math inline">\(H=0\)</span>. What happens in the formula is that <span class="math inline">\(log2(1) = 0\)</span>, and <span class="math inline">\(log2(0)= -infinity\)</span>. By convention, the negative infinities are turned into 0s, which result in a sum of 0s, such that <span class="math inline">\(H=0\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">Events</th>
<th style="text-align: left;"><span class="math inline">\(i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(log_2 P(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(x_i)*log_2 P(x_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">-Inf</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">-Inf</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">D</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">-Inf</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sum</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">H</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(-1*\sum_\text{i=1}^n\)</span></td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="summary" class="level4" data-number="7.6.6">
<h4 data-number="7.6.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.6.6</span> Summary</h4>
<p>In the next sections we will see examples of how Shannon’s information theory was applied in psychology. To summarize the preceding sections, here is what we have learned.</p>
<p>Shannon characterized communication as involving an information channel that transmits discrete messages from a sender to a receiver. He measured the amount of information transmitted in terms of bits. Messages with more unpredictable events transmit more information, measured in bits, than messages with more predictable events. A message that contains 100% predictable events transmits no information because the receiver does not need to receive the message to know what the contents are. A message that is 100% random transmits the maximal amount of information, because the receiver is unable to predict the events, and “learns” something new about the message every time. Messages with intermediate levels of predictability transmit an intermediate amount of information between 0 (maximum predictable) and the number of bits (maximum uncertainty) representing all possible events in the message.</p>
</section>
</section>
<section id="hick-hyman-law" class="level3" data-number="7.7">
<h3 data-number="7.7" class="anchored" data-anchor-id="hick-hyman-law"><span class="header-section-number">7.7</span> Hick-Hyman “Law”</h3>
<p>Let’s connect back to two previous issues. First, we have been discussing mechanistic metaphors of cognition. Shannon’s ideas about communication along an information channel were applied as metaphors that had potential to provide insight cognitive processes. Second, in our previous discussions of research we saw examples of reaction time methods. This section discusses the Hick-Hyman “Law”, which was a very promising application of information theory to findings in the study of choice reaction-times. The word “Law” is quoted because the findings are referred to this way in the literature, but there are edge-cases where the law does not always hold up.</p>
<section id="choice-reaction-time" class="level4" data-number="7.7.1">
<h4 data-number="7.7.1" class="anchored" data-anchor-id="choice-reaction-time"><span class="header-section-number">7.7.1</span> Choice Reaction time</h4>
<p>A basic choice reaction time (CRT) task was already described in the Donders section. Here it is again. In a choice reaction study a participant is presented with one stimulus per trial, and instructed to identify the stimulus as quickly and accurately as possible by making a unique response to the stimulus. For example, the stimulus could be an X or an O, and the response could be to say “X” out loud (when an X is presented), or to say “O” out loud (when an O is presented). The response could be made in different ways too. For example, the buttons on a computer keyboard are often used in reaction time studies, so the “X” on a keyboard could be used to respond to an X, and the “O” on a keyboard could be used to respond to an O.</p>
<p>There are many variations on the general choice reaction time procedure. For example, if you were the experimenter, you could manipulate the kind of stimuli presented, the number of trials, the number of times a stimulus is presented, the kind of response made to each stimulus. You could make the stimuli more difficult to identify by degrading them, you could make the responses easier or harder to make by making the required movements more or less natural. You could give people more or less practice, and measure how consistent they are over time, or whether they get faster or not. All of these kinds of manipulations and more have been conducted and reported in the choice reaction time literature. As you might imagine, this research enterprise produces many patterns of data, because it turns out that several factors make reaction times faster or slower.</p>
<p>When researchers are presented with an overwhelming amount of data, they often look for regular patterns in the data. This helps to summarize the data into more manageable and understandable units. This practice of looking for prominent and predictable patterns in data is similar to the process of identifying “laws” about a natural phenomena. During the behaviorism period there was interest in discovering laws of behavior, and this same interest applied to research on choice reaction time. For example, a general question was, “what are the laws governing choice-reaction time?”. What makes choice-RT faster or slower? Before Hick and Hyman came along, prior work had produced one very reliable finding.</p>
</section>
<section id="the-number-of-alternatives-increases-choice-rt" class="level4" data-number="7.7.2">
<h4 data-number="7.7.2" class="anchored" data-anchor-id="the-number-of-alternatives-increases-choice-rt"><span class="header-section-number">7.7.2</span> The number of alternatives increases choice-RT</h4>
<p>The law-like finding was that choice reaction time increased as the number of alternatives increased. For example, in a choice-RT study it is possible to vary the number of unique stimulus-response pairs. A task could have two stimuli and two responses, or four stimuli and four responses, or any number of stimuli and corresponding responses. The set of possible stimulus-response pairs are called the alternatives <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>When researchers manipulated the number of alternatives in the task, they found that average response time to respond to any stimulus goes up and up, as the number of alternatives increases.</p>
</section>
<section id="number-of-alternatives-or-information-in-the-message" class="level4" data-number="7.7.3">
<h4 data-number="7.7.3" class="anchored" data-anchor-id="number-of-alternatives-or-information-in-the-message"><span class="header-section-number">7.7.3</span> Number of alternatives or “Information” in the message?</h4>
<p><a href="https://en.wikipedia.org/wiki/W._E._Hick">William E. Hick</a> (1912 – 1974) was a British psychologist <span class="citation" data-cites="hollandOriginsBritishCybernetics2011">(and member of the British cybernetics “Ratio club,” <a href="#ref-hollandOriginsBritishCybernetics2011" role="doc-biblioref">Holland &amp; Husbands, 2011</a>)</span>, and <a href="https://en.wikipedia.org/wiki/Ray_Hyman">Ray Hyman</a> an American psychologist, who both had the insight to examine findings in the choice reaction-time literature from the perspective of Shannon’s information theory. They both developed experimental methods to test the idea that people might be sensitive to the amount of “information” in stimulus set, rather than simply the number of alternatives.</p>
<p>The metaphor was that the experimenter is sending information to the subject over the course of the choice reaction time experiment. One question was whether the subjects’ ability to process the message was influenced by the amount of information in the message. Prior research had already shown findings that were consistent with an information theory interpretation.</p>
<p>For example, choice reaction times were faster when there were two alternatives compared to four alternative. Typically, stimuli were presented randomly on each trial. So, from Shannon’s information theory, a task with two unpredictable alternatives carried 1 bit of information, but a task with four unpredictable alternatives carried 2 bits of information. The question for Hick and Hyman was whether increasing the number of alternatives (2 vs 4) was slowing people down for some reason, or whether it was really increases to the amount of information (1 bit vs 2 bits) that was slowing people down (perhaps because more information required more processing time). The problem was that the number of alternatives and amount of information in bits was completely confounded in prior experiments. The solution was to conduct new experiments that de-confounded the relationship between number of alternatives and amount of information.</p>
</section>
<section id="deconfounding-alternatives-from-information" class="level4" data-number="7.7.4">
<h4 data-number="7.7.4" class="anchored" data-anchor-id="deconfounding-alternatives-from-information"><span class="header-section-number">7.7.4</span> Deconfounding alternatives from information</h4>
<p>The solution was to create different versions of a choice-reaction time task that independently varied the number alternatives and the amount of information. In this kind of experiment it would be possible to determine whether reaction times were being influenced by the number of alternative, the amount of information, or both.</p>
<p>We have already seen some ways to manipulate the amount of information in a message. One is to change the number of unique alternatives sent in the message, and the other is to change the predictability of each of event in the message. For example, in the preceding section we saw that communication with four equi-probable events (A, B, C, D) required 2 bits of information. However, if one of the events (A) was more probable than the others, the number of bits information was reduced. As a result, it would be possible to have different versions of a choice reaction time task that held the number of alternatives constant (e.g., four), but changed the predictability of the alternatives, which would vary the amount of information in the task.</p>
</section>
<section id="the-experiments" class="level4" data-number="7.7.5">
<h4 data-number="7.7.5" class="anchored" data-anchor-id="the-experiments"><span class="header-section-number">7.7.5</span> The experiments</h4>
<p>Hick published his results in 1952 <span class="citation" data-cites="hickRateGainInformation1952">(<a href="#ref-hickRateGainInformation1952" role="doc-biblioref">Hick, 1952</a>)</span>, and Hyman published his in 1953 <span class="citation" data-cites="hymanStimulusInformationDeterminant1953">(<a href="#ref-hymanStimulusInformationDeterminant1953" role="doc-biblioref">Hyman, 1953</a>)</span>. We will take a quick look at Hyman’s experimental conditions and findings.</p>
<section id="experiment-i-hyman-1953" class="level5" data-number="7.7.5.1">
<h5 data-number="7.7.5.1" class="anchored" data-anchor-id="experiment-i-hyman-1953"><span class="header-section-number">7.7.5.1</span> Experiment I (Hyman 1953)</h5>
<p>The first experiment was a choice-reaction time task with 8 different conditions, corresponding to the number of alternatives, from 1 to 8. In each condition, all stimuli were presented randomly. Thus, the amount of bits in each condition was ranged from 0 to 3 (bits for 1 to 8 alternatives are: 0, 1, 1.58, 2, 2,32, 2,58, 2,81, and 3). As others had found, Hyman’s subjects showed a strong linear relationship between bits and reaction time. Reaction time increased linearly with amount of bits. Note, however, in Experiment I, the number of alternatives, was completely confounded with the amount of bits.</p>
</section>
<section id="experiment-ii-hyman-1953" class="level5" data-number="7.7.5.2">
<h5 data-number="7.7.5.2" class="anchored" data-anchor-id="experiment-ii-hyman-1953"><span class="header-section-number">7.7.5.2</span> Experiment II (Hyman 1953)</h5>
<p>The second experiment varied the amount information in bits and the number of alternatives separately across 8 conditions. <a href="#fig-7Hyman_e2">Figure&nbsp;11</a> shows the design.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Hyman_e2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Hyman_e2.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;11: The design of the second experiment from <span class="citation" data-cites="hymanStimulusInformationDeterminant1953">Hyman (<a href="#ref-hymanStimulusInformationDeterminant1953" role="doc-biblioref">1953</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>The first two conditions both had 2 alternatives each, however, the choices were more predictable in the first than second condition. In condition 1, the first alternative occurred more often (9/10 times) than the second alternative (1/10 times). In condition 2, the first alternative still occurred more often than the second, but was slightly less predictable (8/10 vs 2/10 times). Using Shannon’s formula to calculate the number bits in each condition, Hyman reports .47 bits for condition 1, and .72 bits for condition 2. If reaction times are influenced by the number of alternatives, then they should be the same in condition 1 and 2, because they both had the same number of alternatives (two each). If reaction times are influenced by the amount of information (measured in bits), then they should be slower in condition 2 compared to condition 1, because condition 2 required more bits (it was less predictable).</p>
<p>The table shows six other conditions. Hyman constructed similar conditions for four, six and eight alternatives. For example, conditions 3 vs 5 both had four alternatives, but condition 5 had more bits (1.99) because the individual choices were less predictable. Similarly, conditions 4 and 6 both had six alternatives, but condition six had more bits because it’s alternatives were less predictable. Finally, conditions 7 and 8 both had eight alternatives, but condition 8 was more unpredictable than condition 7.</p>
</section>
<section id="the-results" class="level5" data-number="7.7.5.3">
<h5 data-number="7.7.5.3" class="anchored" data-anchor-id="the-results"><span class="header-section-number">7.7.5.3</span> The results</h5>
<p><a href="#fig-7Hick_hyman_law">Figure&nbsp;12</a> shows the results for two (of four) subjects reported by Hyman. Hyman had conducted a third experiment where he manipulated the amount of information separately from the number of alternatives in a slightly different way; and these results are also included in the plot.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-7Hick_hyman_law" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="imgs/Hick_hyman_law.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;12: The design of the second experiment from <span class="citation" data-cites="hymanStimulusInformationDeterminant1953">Hyman (<a href="#ref-hymanStimulusInformationDeterminant1953" role="doc-biblioref">1953</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>All told, his subjects completed three experiments worth of choice reaction time experiments. Each experiment had different numbers of alternatives, and separately manipulated amounts of information measured in bits. The big finding can be stated by the Hick-Hyman Law: choice-reaction time increased as a linear function of the information (measured in bits) in the stimulus set. Critically, it was not simply the number of number alternatives that was making people slower. Instead, people were apparently responding to the amount of information in the stimulus set.</p>
</section>
</section>
<section id="implications-for-behaviorism" class="level4" data-number="7.7.6">
<h4 data-number="7.7.6" class="anchored" data-anchor-id="implications-for-behaviorism"><span class="header-section-number">7.7.6</span> Implications for Behaviorism</h4>
<p>I could imagine some behaviorists being disturbed by the results of Hick and Hyman’s findings. On the one hand, they should have been very impressed because the results appeared so orderly and lawful. On the other hand, the problem with the Hick-Hyman law was that it violated ideological aspects of behaviorist assumptions. For example, behaviorists were interested in understanding the lawful regularities connecting a stimulus with a subsequent response. But, consider what Hick and Hyman had shown. Their finding was that a response to a stimulus <strong>did not depend on the stimulus that was presented</strong>; instead, the speed of responding to the presented stimulus apparently depended <strong>on the predictability of the other stimuli that could have been presented instead</strong>.</p>
<p>In other words, people were not only responding to a stimulus, they were also responding to all the other stimuli that were not presented. Or, we could say that responses to one stimulus were being influenced by expectations about other stimuli in the set of possible stimuli. Thus, the Hick-Hyman law was a complicating factor for theories of behaviorism that were not acknowledging a role for non-presented stimuli and/or expectations about stimuli to influence behavior. Furthermore, the whole information processing metaphor was a step away from behaviorism, because it implied the existence of intervening mental operations between a stimulus and response–and, it came along with a mathematical system in Shannon’s communication theory, that could potentially be useful for describing the nature of the mental operations.</p>
</section>
<section id="debate-about-interpretation" class="level4" data-number="7.7.7">
<h4 data-number="7.7.7" class="anchored" data-anchor-id="debate-about-interpretation"><span class="header-section-number">7.7.7</span> Debate about interpretation</h4>
<p>Hick and Hyman were not the only psychologists incorporating information theory into psychology, and their example was chosen to fit with the theme of reaction time measurements in this chapter. The 1950s saw many psychologists relate their observations in terms of information theory. However, the application of information theory to psychology also fell out of favor for several reasons in the 1960s, even though it continued to re-appear throughout the decades, and remains an important tool in modern cognitive research.</p>
<section id="information-theory-was-not-a-psychological-theory" class="level5" data-number="7.7.7.1">
<h5 data-number="7.7.7.1" class="anchored" data-anchor-id="information-theory-was-not-a-psychological-theory"><span class="header-section-number">7.7.7.1</span> Information theory was not a psychological theory</h5>
<p>A primary issue was that “information theory” was not a theory of psychological process. It was just simple mathematical formula to summarize the uncertainty (information) in a set of stimuli. It was interesting that reaction times were linearly related to the uncertainty in the stimulus set, but this observation in and of itself did not explain the mechanism. What was causing reaction times to follow the “information” in the stimulus set? Information theory did not provide an answer, it just provided a measure.</p>
</section>
<section id="the-hick-hyman-law-could-be-violated" class="level5" data-number="7.7.7.2">
<h5 data-number="7.7.7.2" class="anchored" data-anchor-id="the-hick-hyman-law-could-be-violated"><span class="header-section-number">7.7.7.2</span> The Hick-Hyman law could be violated</h5>
<p>The potential discovery of law relating stimulus set information to reaction time performance generated interest among other researchers, and it didn’t take very long for the Hick-Hyman law to show some inconsistencies <span class="citation" data-cites="proctorHickLawChoice2018">(for a review see, <a href="#ref-proctorHickLawChoice2018" role="doc-biblioref">Proctor &amp; Schneider, 2018</a>)</span>.</p>
<p>One example was the role of practice, which leads to performance improvements in most tasks including choice-reaction time tasks. Teichner &amp; Krebs reviewed the literature on practice effects in 1974 <span class="citation" data-cites="teichnerLawsVisualChoice1974">(<a href="#ref-teichnerLawsVisualChoice1974" role="doc-biblioref">Teichner &amp; Krebs, 1974</a>)</span>, and suggested that the Hick-Hyman law may not apply for highly practiced subjects. For example, a highly practiced subject would be fast in all conditions, no matter how many alternatives there were. In these cases, reaction-time performance would not depend on the amount of information in the stimulus-set, but instead on factors to do with practice.</p>
<p>Another example was the supposed linear relationship between number of bits and reaction-time. Most studies had used a small range of alternatives (2 - 32, or 1-5 bits). In 1963, Siebel <span class="citation" data-cites="seibelDiscriminationReactionTime1962">(<a href="#ref-seibelDiscriminationReactionTime1962" role="doc-biblioref">Seibel, 1962</a>)</span> created a task that had up to 1032 alternatives, and found that practiced subjects showed very little difference in reaction times compared to a task with 31 alternatives. He suggested the Hick-Hyman Law might only apply to a small range of set-sizes that was common in the literature. But, in 1966, Hilgendorf <span class="citation" data-cites="hillgendorfInformationInputResponse1966">(<a href="#ref-hillgendorfInformationInputResponse1966" role="doc-biblioref">Hillgendorf, 1966</a>)</span> used yet another task capable of presenting 1000 alternatives, and found that the Hick-Hyman law did show a linear relationship with reaction time, even across the large range in set-size.</p>
<p>As a whole, the choice reaction time procedure is similar to the PRP paradigm. It has generated hundreds of experiments and idiosyncratic findings, and by now, many different theories and explanations of the results. Below, we consider a few explanations of Hick and Hyman’s findings.</p>
</section>
<section id="hicks-explanations" class="level5" data-number="7.7.7.3">
<h5 data-number="7.7.7.3" class="anchored" data-anchor-id="hicks-explanations"><span class="header-section-number">7.7.7.3</span> Hick’s explanations</h5>
<p>Hick considered four categories of explanations for the finding that reaction times increase as a function of uncertainty in the stimulus set. And, they were all different from the behaviorist approach we saw last chapter. For example, Hick used mechanistic metaphors to describe possible operations that might be taking place in between a stimulus-response, and he considered how these operations might be able to account for his data.</p>
<p>One idea was a match-to-template hypothesis. If a person had to identify a stimulus as being one of four stimuli, they could compare it to mental templates of each of the alternatives, and then they could respond as soon as they matched the current stimulus to one of the templates. If the comparison was conducted in serial (one after the other), then the set-size of alternatives should increase the reaction time. This is because people would, on average, need to make more mental comparisons between a perceived stimulus and the additional mental templates. However, the match-to-template hypothesis would only explain why reaction time would increase as a function of set-size, and not as a function of the uncertainty of the alternatives.</p>
<p>Another idea was that people were somehow using binary logic to aid the task of stimulus identification. For example, a single stimulus from a set could be found by progressively applying binary tests. Consider finding one stimulus from a set of eight by asking binary questions with a yes/no answer. Here it takes three bits, or three binary decisions to identify the stimulus. For example:</p>
<ol type="1">
<li>Is the stimulus in the first four items? Yes (if no, then it must be in the last four. We’ll assume it is in the first four)</li>
<li>Is the stimulus in the first two? No, then it must be in the second two (3rd or 4th item).</li>
<li>Is the stimulus the 3rd item? Yes, you have found it; or No, it must be the 4th item, and you have found it.</li>
</ol>
</section>
<section id="a-tale-of-two-confounds-priming-explanations" class="level5" data-number="7.7.7.4">
<h5 data-number="7.7.7.4" class="anchored" data-anchor-id="a-tale-of-two-confounds-priming-explanations"><span class="header-section-number">7.7.7.4</span> A tale of two confounds: Priming Explanations</h5>
<p>The Hick-Hyman law came from experiments attempting to de-confound the influence of set-size (number of alternatives) from the possible influence of stimulus uncertainty (information in bits) on choice reaction-time performance. However, the manipulations to hold set-size constant, and vary predictability of the individual stimuli in the set could create new confounds. For example, when you change stimulus probabilities such that some stimuli occur more than others, you may also change the number of immediate repetitions of two successive stimuli.</p>
<p>Another well-known finding in the reaction time literature is called <em>repetition priming</em>. For example, if you respond to an <em>A</em> stimulus, and then you have to respond to the same stimulus again on the next trial, your response time is typically faster compared to trials when the previous stimulus was not repeated. In other words, <em>repetition priming</em> is the finding of faster responses to a repeated stimulus compared to a non-repeated stimulus.</p>
<p>In contrast to Hick’s explanation of his findings in terms the operation of mental processes, Kornblum suggested the Hick-Hyman law was just an artifact of repetition priming <span class="citation" data-cites="kornblumChoiceReactionTime1967">(<a href="#ref-kornblumChoiceReactionTime1967" role="doc-biblioref">Kornblum, 1967</a>)</span>. For example, conditions with fewer alternatives also tended to have more stimulus repetition trials compared to conditions with more alternatives. Similarly, conditions with more predictable and frequent stimuli tended to have more stimulus repetition trials compared to conditions with less predictable and frequent stimuli. On average, reaction time performance would be faster for conditions that had more immediate repetition trials compared to conditions that did not. This is an example where one finding (the Hick-Hyman law) is “explained” in terms another finding (repetition priming). I quote “explained”, because repetition priming is also a phenomena that requires explanation, and the practice of describing one unexplained phenomena by referring to another unexplained phenomena, although useful, does not produce an explanation of the processes accounting for either finding.</p>
</section>
</section>
</section>
<section id="information-theory-and-beyond" class="level3" data-number="7.8">
<h3 data-number="7.8" class="anchored" data-anchor-id="information-theory-and-beyond"><span class="header-section-number">7.8</span> Information theory and beyond</h3>
<p>The research trajectory of the Hick-Hyman law is a good example of common research patterns in cognitive psychology. Somebody produces a new finding, like the Hick-Hyman law. This new finding generates many experiments to “kick-the-tires” of the basic finding. Several process theories of the finding are also generated to explain the findings. Then further experiments are conducted to test implications of the process theories. At the same time, confounding factors might be identified, and these could suggest that totally different processes might be at play. Very often, the outcome of decades of research results in a large collection of somewhat conflicting findings, many claims about possible confounds and alternative explanations, and also many theoretical process models that go some distance in explaining some of the findings. For example, in their 2018 review of the modern literature stemming from the Hick-Hyman law, Proctor and Schneider <span class="citation" data-cites="proctorHickLawChoice2018">(<a href="#ref-proctorHickLawChoice2018" role="doc-biblioref">Proctor &amp; Schneider, 2018</a>)</span> discuss a large collection of sometimes conflicting findings, and many categories of models that take different approaches to the explain the Hick-Hyman law. They also suggest the findings and the models have the potential to be useful and possibly generate further insights into human performance processes or other processes if they end up becoming a useful metaphor for other domains.</p>
<p>Information theory and the idea of information processing stages will pop-up again in the next chapters because these ideas continue to be used as metaphors in cognitive psychology. In the next chapter, we begin our discussion of memory processes, which in my opinion are fundamental to many other cognitive abilities. After the memory chapters, we will continue to explore how memory processes may be involved with, and potentially explain some aspects of cognition reserved for the later chapters.</p>
</section>
<section id="appendix" class="level3" data-number="7.9">
<h3 data-number="7.9" class="anchored" data-anchor-id="appendix"><span class="header-section-number">7.9</span> Appendix</h3>

</section>


<div id="quarto-appendix" class="default"><section id="references" class="level4 appendix" data-number="7.9.1"><h2 class="quarto-appendix-heading"></h2><div class="quarto-appendix-contents">

<!--

should mention Hick's correlational research with IQ in 1951

Do this later maybe

## Four Ms: Metaphor, Mechanism, Method, and Multiplicity

E.g., if people are responding to "information" in the stimulus, how are they doing it? What is the mechanism how does it work?


-->
<!--
At the end of the chapter I introduce the *four Ms*: Metaphor, Mechanism, Method, and Multiplicity. Cognitive psychology is full of metaphors for cognitive mechanisms, packed with different methods, and although it can be collected under a title like "Cognitive psychology", it can also be viewed as a multiplicity, rather than a mono-dimensional research enterprise with unitary goals. 
-->



</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">7.9.1 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-alexanderDondersDeadCortical2015" class="csl-entry" role="listitem">
Alexander, D. M., Trengove, C., &amp; van Leeuwen, C. (2015). Donders is dead: Cortical traveling waves and the limits of mental chronometry in cognitive neuroscience. <em>Cognitive Processing</em>, <em>16</em>(4), 365–375. <a href="https://doi.org/f7zgts">https://doi.org/f7zgts</a>
</div>
<div id="ref-borelMecaniqueStatiqueIrreversibilite1913" class="csl-entry" role="listitem">
Borel, É. (1913). La mécanique statique et l’irréversibilité. <em>J. Phys. Theor. Appl.</em>, <em>3</em>(1), 189–196. <a href="https://doi.org/c9nnbz">https://doi.org/c9nnbz</a>
</div>
<div id="ref-broadbentMechanicalModelHuman1957" class="csl-entry" role="listitem">
Broadbent, D. E. (1957). A mechanical model for human attention and immediate memory. <em>Psychological Review</em>, <em>64</em>(3), 205. <a href="https://doi.org/d5srnr">https://doi.org/d5srnr</a>
</div>
<div id="ref-broadbentPerceptionCommunication1958" class="csl-entry" role="listitem">
Broadbent, D. E. (1958). <em>Perception and communication</em>. <span>Elsevier</span>.
</div>
<div id="ref-canalesExitFrogEnter2001" class="csl-entry" role="listitem">
Canales, J. (2001). Exit the frog, enter the human: Physiology and experimental psychology in nineteenth-century astronomy. <em>The British Journal for the History of Science</em>, <em>34</em>(2), 173–197. <a href="https://doi.org/cs2z26">https://doi.org/cs2z26</a>
</div>
<div id="ref-craikTheoryHumanOperator1948" class="csl-entry" role="listitem">
Craik, K. J. W. (1948). Theory of the human operator in control systems <span>II</span>. <span>Man</span> as an element in a control system. <em>British Journal of Psychology</em>, <em>38</em>, 142–148.
</div>
<div id="ref-davisHumanOperatorSingle1957" class="csl-entry" role="listitem">
Davis, R. (1957). The human operator as a single channel information system. <em>Quarterly Journal of Experimental Psychology</em>, <em>9</em>(3), 119–129. <a href="https://doi.org/d2n3mw">https://doi.org/d2n3mw</a>
</div>
<div id="ref-dondersSpeedMentalProcesses1868" class="csl-entry" role="listitem">
Donders, F. C. (1868–1969). On the speed of mental processes. In W. G. Koster (Ed.), <em>Attention and <span>Performance II</span></em>. <span>North-Holland Publishing Company</span>.
</div>
<div id="ref-fraissePeriodeRefractairePsychologique1957" class="csl-entry" role="listitem">
Fraisse, P. (1957). La période réfractaire psychologique. <em>L’Année Psychologique</em>, <em>57</em>(2), 315–328. <a href="https://doi.org/dtnd4s">https://doi.org/dtnd4s</a>
</div>
<div id="ref-greenwoodUnderstandingCognitiveRevolution1999" class="csl-entry" role="listitem">
Greenwood, J. D. (1999). Understanding the <span>“cognitive revolution”</span> in psychology. <em>Journal of the History of the Behavioral Sciences</em>, <em>35</em>(1), 1–22. <a href="https://doi.org/ffh4k7">https://doi.org/ffh4k7</a>
</div>
<div id="ref-hickDiscontinuousFunctioningHuman1948" class="csl-entry" role="listitem">
Hick, W. E. (1948). The discontinuous functioning of the human operator in pursuit tasks. <em>Quarterly Journal of Experimental Psychology</em>, <em>1</em>(1), 36–51. <a href="https://doi.org/d4pnsh">https://doi.org/d4pnsh</a>
</div>
<div id="ref-hickRateGainInformation1952" class="csl-entry" role="listitem">
Hick, W. E. (1952). On the rate of gain of information. <em>Quarterly Journal of Experimental Psychology</em>, <em>4</em>(1), 11–26. <a href="https://doi.org/10.1080/17470215208416600">https://doi.org/10.1080/17470215208416600</a>
</div>
<div id="ref-hillgendorfInformationInputResponse1966" class="csl-entry" role="listitem">
Hillgendorf, L. (1966). Information input and response time. <em>Ergonomics</em>, <em>9</em>(1), 31–37. <a href="https://doi.org/dd5k9b">https://doi.org/dd5k9b</a>
</div>
<div id="ref-hollandOriginsBritishCybernetics2011" class="csl-entry" role="listitem">
Holland, O., &amp; Husbands, P. (2011). The origins of <span>British</span> cybernetics: The <span>Ratio Club</span>. <em>Kybernetes</em>, <em>40</em>(1/2), 110–123. <a href="https://doi.org/dn98st">https://doi.org/dn98st</a>
</div>
<div id="ref-hymanStimulusInformationDeterminant1953" class="csl-entry" role="listitem">
Hyman, R. (1953). Stimulus information as a determinant of reaction time. <em>Journal of Experimental Psychology</em>, <em>45</em>(3), 188–196. <a href="https://doi.org/cq3kjd">https://doi.org/cq3kjd</a>
</div>
<div id="ref-jastrowTimerelationsMentalPhenomena1890" class="csl-entry" role="listitem">
Jastrow, J. (1890). <em>The time-relations of mental phenomena</em>. <span>NDC Hodges</span>.
</div>
<div id="ref-kornblumChoiceReactionTime1967" class="csl-entry" role="listitem">
Kornblum, S. (1967). Choice reaction time for repetitions and non-repetitions: <span>A</span> re-examination of the information hypothesis. <em>Acta Psychologica</em>, <em>27</em>, 178–187. <a href="https://doi.org/dqkb2g">https://doi.org/dqkb2g</a>
</div>
<div id="ref-kosterPreface1969" class="csl-entry" role="listitem">
Koster, W. G. (1969). Preface. In W. G. Koster (Ed.), <em>Attention and <span>Performance II</span>: Proceedings of the donders centenary symposium on reaction time</em>. <span>North-Holland Publishing Company</span>.
</div>
<div id="ref-mandlerOriginsCognitiveEvolution23" class="csl-entry" role="listitem">
Mandler, G. (2002). Origins of the cognitive (r)evolution. <em>Journal of the History of the Behavioral Sciences</em>, <em>38</em>(4), 339–353. <a href="https://doi.org/d4grcn">https://doi.org/d4grcn</a>
</div>
<div id="ref-millerCognitiveRevolutionHistorical2003" class="csl-entry" role="listitem">
Miller, G. A. (2003). The cognitive revolution: A historical perspective. <em>Trends in Cognitive Sciences</em>, <em>7</em>(3), 141–144. <a href="https://doi.org/b7wqdk">https://doi.org/b7wqdk</a>
</div>
<div id="ref-neisserCognitivePsychology1967" class="csl-entry" role="listitem">
Neisser, U. (1967). <em>Cognitive psychology</em>. <span>Meredith Publishing Company</span>.
</div>
<div id="ref-proctorHickLawChoice2018" class="csl-entry" role="listitem">
Proctor, R. W., &amp; Schneider, D. W. (2018). Hick’s law for choice reaction time: <span>A</span> review. <em>Quarterly Journal of Experimental Psychology</em>, <em>71</em>(6), 1281–1299. <a href="https://doi.org/gfs37w">https://doi.org/gfs37w</a>
</div>
<div id="ref-seibelDiscriminationReactionTime1962" class="csl-entry" role="listitem">
Seibel, R. (1962). Discrimination reaction time as a function of the number of stimulus-response pairs and the self-pacing adjustment of the subject. <em>Psychological Monographs: General and Applied</em>, <em>76</em>(42), 1. <a href="https://doi.org/b9gnjk">https://doi.org/b9gnjk</a>
</div>
<div id="ref-shannonSymbolicAnalysisRelay1938" class="csl-entry" role="listitem">
Shannon, C. E. (1938). A symbolic analysis of relay and switching circuits. <em>Electrical Engineering</em>, <em>57</em>(12), 713–723. <a href="https://doi.org/ggztcn">https://doi.org/ggztcn</a>
</div>
<div id="ref-shannonAlgebraTheoreticalGenetics1940" class="csl-entry" role="listitem">
Shannon, C. E. (1940). <em>An algebra for theoretical genetics</em> [PhD thesis]. <span>Massachusetts Institute of Technology</span>.
</div>
<div id="ref-shannonMathematicalTheoryCommunication1949" class="csl-entry" role="listitem">
Shannon, C. E., &amp; Weaver, W. (1949). <em>The mathematical theory of communication</em>. <span>University of Illinois press</span>. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=IZ77BwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=related:neNAI2dkLQcJ:scholar.google.com/&amp;ots=hknDdVuK4v&amp;sig=8pbhXsHmouktq7OVKrUO4KLT2NQ">https://books.google.com/books?hl=en&amp;lr=&amp;id=IZ77BwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=related:neNAI2dkLQcJ:scholar.google.com/&amp;ots=hknDdVuK4v&amp;sig=8pbhXsHmouktq7OVKrUO4KLT2NQ</a>
</div>
<div id="ref-sperryImpactPromiseCognitive1993" class="csl-entry" role="listitem">
Sperry, R. W. (1993). The impact and promise of the cognitive revolution. <em>American Psychologist</em>, <em>48</em>(8), 878. <a href="https://doi.org/cr7svp">https://doi.org/cr7svp</a>
</div>
<div id="ref-sternbergDiscoveryProcessingStages1969" class="csl-entry" role="listitem">
Sternberg, S. (1969). The discovery of processing stages: <span>Extensions</span> of <span>Donders</span>’ method. <em>Acta Psychologica</em>, <em>30</em>, 276–315. <a href="https://doi.org/ftg756">https://doi.org/ftg756</a>
</div>
<div id="ref-teichnerLawsVisualChoice1974" class="csl-entry" role="listitem">
Teichner, W. H., &amp; Krebs, M. J. (1974). Laws of visual choice reaction time. <em>Psychological Review</em>, <em>81</em>(1), 75. <a href="https://doi.org/br3tcg">https://doi.org/br3tcg</a>
</div>
<div id="ref-umplebyDefinitionsCybernetics2008" class="csl-entry" role="listitem">
Umpleby, S. (2008). Definitions of cybernetics. <em>The Larry Richards Reader 1997–2007</em>, 9–11.
</div>
<div id="ref-welfordPsychologicalRefractoryPeriod1952" class="csl-entry" role="listitem">
Welford, A. T. (1952). The ‘psychological refractory period’and the timing of high-speed performance—a review and a theory. <em>British Journal of Psychology. General Section</em>, <em>43</em>(1), 2–19. <a href="https://doi.org/c7jfrb">https://doi.org/c7jfrb</a>
</div>
<div id="ref-welfordEvidenceSinglechannelDecision1959" class="csl-entry" role="listitem">
Welford, A. T. (1959). Evidence of a single-channel decision mechanism limiting performance in a serial reaction task. <em>Quarterly Journal of Experimental Psychology</em>, <em>11</em>(4), 193–210. <a href="https://doi.org/dx8xfc">https://doi.org/dx8xfc</a>
</div>
<div id="ref-wienerCyberneticsControlCommunication1948" class="csl-entry" role="listitem">
Wiener, N. (1948). <em>Cybernetics or <span>Control</span> and <span>Communication</span> in the <span>Animal</span> and the <span>Machine</span></em>. <span>MIT press</span>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>hmmm…<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>this is also why H refers to entropy, which is a physics concept for randomness or disorder in a system<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>as in Borges <a href="https://en.wikipedia.org/wiki/The_Library_of_Babel">Library of Babel</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>the choice-RT task is also called an N-AFC task, where N specifies the number of alternatives (A) in the forced choice (FC) task.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">https://creativecommons.org/licenses/by-sa/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@incollection{crump2021,
  author = {Crump, Matthew J. C.},
  editor = {Crump, Matthew J. C.},
  title = {Information {Processing}},
  booktitle = {Instances of Cognition: Questions, Methods, Findings,
    Explanations, Applications, and Implications},
  date = {2021-09-01},
  url = {https://crumplab.com/cognition/textbook},
  langid = {en},
  abstract = {This chapter covers the concepts of processing stages,
    information, and capacity limitations, which became popular
    cognitive research topics around the 1950s and 60s.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-crump2021" class="csl-entry quarto-appendix-citeas" role="listitem">
Crump, M. J. C. (2021). Information Processing. In M. J. C. Crump (Ed.),
<em>Instances of Cognition: Questions, Methods, Findings, Explanations,
Applications, and Implications</em>. <a href="https://crumplab.com/cognition/textbook">https://crumplab.com/cognition/textbook</a>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">This page is built with <a href="https://quarto.org/">Quarto</a>.</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../LICENSE.md">License</a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>